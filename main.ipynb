{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Jupyter environment detected. Enabling Open3D WebVisualizer.\n",
      "[Open3D INFO] WebRTC GUI backend enabled.\n",
      "[Open3D INFO] WebRTCWindowSystem: HTTP handshake server disabled.\n"
     ]
    }
   ],
   "source": [
    "import cv2\n",
    "import json\n",
    "import torch\n",
    "import urllib.request\n",
    "from ultralytics import YOLO\n",
    "import open3d\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import open3d as o3d\n",
    "from scipy.ndimage import gaussian_filter"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Pagine utili del libro:\n",
    "693 \n",
    "https://www.youtube.com/watch?v=KOSS24P3_fY\n",
    "\n",
    "TODO list:\n",
    "\n",
    "-stimare la posa della camera a ogni frame usando l'8 point algorithm (sfrutta 8 corrispondenza tra punti 2D nei due frame per calcolare la relazione epipolare (relazione che lega due immagini acquisite da due punti di vista differenti))\n",
    "\n",
    "-calcolare i punti della scena in coordinate globali sfruttando la relazione x_globale = R * x_camera + t\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Parametri intrinseci della fotocamera"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Parametri intrinseci della fotocamera\n",
    "fvideo = cv2.VideoCapture(\"./references/video/cardrive.mp4\")\n",
    "\n",
    "ret, fframe = fvideo.read()\n",
    "fx = 500  # Lunghezza focale in pixel sull'asse x\n",
    "fy = 500  # Lunghezza focale in pixel sull'asse y\n",
    "cx = fframe.shape[1] // 2  # Centro ottico sull'asse x (al centro dell'immagine)\n",
    "cy = fframe.shape[0] // 2  # Centro ottico sull'asse y (al centro dell'immagine)\n",
    "fvideo.release()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "apertura file json"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('class_to_img.json', 'r') as file:\n",
    "    classtoimg = json.load(file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "0: 640x480 4 persons, 1 bus, 1 stop sign, 42.0ms\n",
      "Speed: 3.0ms preprocess, 42.0ms inference, 45.0ms postprocess per image at shape (1, 3, 640, 480)\n",
      "tensor([1., 2., 3., 4., 5., 6.])\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# Carica il modello YOLO preaddestrato (puoi sostituirlo con un modello addestrato su un tuo dataset)\n",
    "yolo_model = YOLO('yolov8n.pt') \n",
    "\n",
    "# Carica un frame e rileva gli oggetti\n",
    "#frame = cv2.imread('bus.jpg')\n",
    "#results = yolo_model.track(frame)\n",
    "\n",
    "# Visualizza i risultati\n",
    "\n",
    "#annotated_frame = results[0].plot()\n",
    "#print(results[0].boxes.id)\n",
    "\n",
    "#cv2.imshow(\"si\", annotated_frame)\n",
    "#cv2.waitKey(0)\n",
    "\n",
    "#cv2.destroyAllWindows()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Istanziare il modello di stima della profondita'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using cache found in C:\\Users\\Edoardo/.cache\\torch\\hub\\intel-isl_MiDaS_master\n",
      "c:\\Users\\Edoardo\\anaconda3\\envs\\myenv\\Lib\\site-packages\\timm\\models\\layers\\__init__.py:48: FutureWarning: Importing from timm.models.layers is deprecated, please import via timm.layers\n",
      "  warnings.warn(f\"Importing from {__name__} is deprecated, please import via timm.layers\", FutureWarning)\n",
      "Using cache found in C:\\Users\\Edoardo/.cache\\torch\\hub\\intel-isl_MiDaS_master\n"
     ]
    }
   ],
   "source": [
    "\n",
    "model_type = \"DPT_Large\"\n",
    "\n",
    "midas = torch.hub.load(\"intel-isl/MiDaS\", model_type)\n",
    "\n",
    "device = torch.device(\"cuda\") if torch.cuda.is_available() else torch.device(\"cpu\")\n",
    "midas.to(device)\n",
    "midas.eval()\n",
    "\n",
    "midas_transforms = torch.hub.load(\"intel-isl/MiDaS\", \"transforms\")\n",
    "\n",
    "if model_type == \"DPT_Large\":\n",
    "    transform = midas_transforms.dpt_transform\n",
    "else:\n",
    "    transform = midas_transforms.small_transform\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "True\n"
     ]
    }
   ],
   "source": [
    "print(torch.cuda.is_available())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "30\n"
     ]
    }
   ],
   "source": [
    "video_path = \"./references/video/cardrive.mp4\"\n",
    "cap = cv2.VideoCapture(video_path)\n",
    "\n",
    "# Ottieni il framerate (FPS) del video\n",
    "fps = int(cap.get(cv2.CAP_PROP_FPS))\n",
    "print(fps)\n",
    "frame_time = int(1000/fps)\n",
    "frame_count = 0\n",
    "\n",
    "\n",
    "while cap.isOpened():\n",
    "\n",
    "    # Leggi il frame corrente\n",
    "    ret, frame = cap.read()\n",
    "    if not ret:\n",
    "        break  # Esci se il video Ã¨ terminato\n",
    "    cv2.imshow('frame', frame)\n",
    "\n",
    "    if cv2.waitKey(15) & 0xFF == ord('q'):\n",
    "        break\n",
    "\n",
    "cap.release()\n",
    "cv2.destroyAllWindows()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "definizione della funzione midas_elaboration da inserire nel ciclo della gestione dei frame del video"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [],
   "source": [
    "def midas_elaboration(frame):\n",
    "    frame = cv2.cvtColor(frame, cv2.COLOR_BGR2RGB)\n",
    "    input_batch = transform(frame).to(device)\n",
    "    \n",
    "    # Elabora il frame (aggiungi qui il tuo codice di elaborazione)\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        prediction = midas(input_batch)\n",
    "\n",
    "        prediction = torch.nn.functional.interpolate(\n",
    "            prediction.unsqueeze(1),\n",
    "            size=frame.shape[:2],\n",
    "            mode=\"bicubic\",\n",
    "            align_corners=False,\n",
    "        ).squeeze()\n",
    "    output = prediction.cpu().numpy()\n",
    "    output_normalized = cv2.normalize(output, None, 0, 255, cv2.NORM_MINMAX)\n",
    "    output_normalized = output_normalized.astype(np.uint8)\n",
    "    #cv2.imshow('frame', output_normalized)\n",
    "    return output_normalized"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "definizione della funzione yolo_elaboration da inserire nel ciclo della gestione dei frame del video"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "combinazione dei risultati delle due funzioni precedenti per calcolare la distanza stimata degli oggetti rilevati nel frame"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def elaboration(midas, yolo, coefficiente, old_detection_info):\n",
    "    detection_info = yolo\n",
    "    depth_map = midas\n",
    "    npframe = np.array(depth_map)\n",
    "    updated_detection_info = []\n",
    "    for object in detection_info:\n",
    "        obj_id = object['id'] \n",
    "        label = object['label']\n",
    "        confidence = object['confidence']\n",
    "        x1 = object['x1']\n",
    "        x2 = object['x2']\n",
    "        y1 = object['y1']\n",
    "        y2 = object['y2']\n",
    "        if confidence < 0.50:\n",
    "            continue\n",
    "        object_data = []\n",
    "        for y in range(y1, y2):\n",
    "            for x in range(x1, x2):\n",
    "                depth = depth_map[y,x]\n",
    "                real_depth = coefficiente / depth\n",
    "                real_x = ((x - cx) * real_depth)/fx\n",
    "                real_y = ((y - cy) * real_depth)/fy\n",
    "                object_data.append((real_x, real_y, real_depth))\n",
    "        updated_detection_info.append({\n",
    "            'id' : obj_id,\n",
    "            'label' : label,\n",
    "            'confidence' : confidence,\n",
    "            '3dcoord' : object_data\n",
    "        })\n",
    "    return updated_detection_info\n",
    "        \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "definizione della funzione threedMapConversion da inserire nel ciclo della gestione dei frame del video per inserire gli oggetti rilevati in una mappa 3d"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "def threedMapConversion(info, detected):\n",
    "    points = []\n",
    "    for object in info:\n",
    "        \n",
    "        ##obj_class = object['label']\n",
    "        ##img = classtoimg.get(obj_class, \"Percorso non trovato\")\n",
    "        threedcoords = object['3dcoord']\n",
    "        points.extend(threedcoords)\n",
    "    points = np.array(points)\n",
    "    pcd = o3d.geometry.PointCloud()\n",
    "    pcd.points = o3d.utility.Vector3dVector(points)\n",
    "    ##o3d.visualization.draw_geometries([pcd])\n",
    "    vis = o3d.visualization.Visualizer()\n",
    "    vis.create_window()\n",
    "    vis.add_geometry(pcd)\n",
    "    ctr = vis.get_view_control()\n",
    "    parameters = o3d.io.read_pinhole_camera_parameters(\"ScreenCamera_2024-11-26-20-48-41.json\")\n",
    "    ctr.convert_from_pinhole_camera_parameters(parameters, True)\n",
    "    vis.run()\n",
    "    vis.destroy_window()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "ciclo di gestione dei frame del video"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "parte midas completata\n",
      "\n",
      "0: 384x640 4 cars, 6.0ms\n",
      "Speed: 1.0ms preprocess, 6.0ms inference, 1.5ms postprocess per image at shape (1, 3, 384, 640)\n",
      "parte yolo completata\n",
      "parte generale completata\n",
      "parte midas completata\n",
      "\n",
      "0: 384x640 4 cars, 5.5ms\n",
      "Speed: 1.0ms preprocess, 5.5ms inference, 1.5ms postprocess per image at shape (1, 3, 384, 640)\n",
      "parte yolo completata\n",
      "parte generale completata\n",
      "parte midas completata\n",
      "\n",
      "0: 384x640 5 cars, 6.0ms\n",
      "Speed: 1.0ms preprocess, 6.0ms inference, 1.0ms postprocess per image at shape (1, 3, 384, 640)\n",
      "parte yolo completata\n",
      "parte generale completata\n",
      "parte midas completata\n",
      "\n",
      "0: 384x640 6 cars, 6.0ms\n",
      "Speed: 1.0ms preprocess, 6.0ms inference, 1.5ms postprocess per image at shape (1, 3, 384, 640)\n",
      "parte yolo completata\n",
      "parte generale completata\n",
      "parte midas completata\n",
      "\n",
      "0: 384x640 7 cars, 6.0ms\n",
      "Speed: 1.5ms preprocess, 6.0ms inference, 1.5ms postprocess per image at shape (1, 3, 384, 640)\n",
      "parte yolo completata\n",
      "parte generale completata\n",
      "parte midas completata\n",
      "\n",
      "0: 384x640 5 cars, 6.0ms\n",
      "Speed: 1.0ms preprocess, 6.0ms inference, 1.0ms postprocess per image at shape (1, 3, 384, 640)\n",
      "parte yolo completata\n",
      "parte generale completata\n",
      "parte midas completata\n",
      "\n",
      "0: 384x640 5 cars, 6.0ms\n",
      "Speed: 1.0ms preprocess, 6.0ms inference, 1.5ms postprocess per image at shape (1, 3, 384, 640)\n",
      "parte yolo completata\n",
      "parte generale completata\n",
      "parte midas completata\n",
      "\n",
      "0: 384x640 5 cars, 6.0ms\n",
      "Speed: 1.5ms preprocess, 6.0ms inference, 1.0ms postprocess per image at shape (1, 3, 384, 640)\n",
      "parte yolo completata\n",
      "parte generale completata\n",
      "parte midas completata\n",
      "\n",
      "0: 384x640 6 cars, 6.0ms\n",
      "Speed: 1.0ms preprocess, 6.0ms inference, 1.0ms postprocess per image at shape (1, 3, 384, 640)\n",
      "parte yolo completata\n",
      "parte generale completata\n",
      "parte midas completata\n",
      "\n",
      "0: 384x640 7 cars, 6.0ms\n",
      "Speed: 1.0ms preprocess, 6.0ms inference, 1.0ms postprocess per image at shape (1, 3, 384, 640)\n",
      "parte yolo completata\n",
      "parte generale completata\n",
      "parte midas completata\n",
      "\n",
      "0: 384x640 5 cars, 6.0ms\n",
      "Speed: 1.0ms preprocess, 6.0ms inference, 1.0ms postprocess per image at shape (1, 3, 384, 640)\n",
      "parte yolo completata\n",
      "parte generale completata\n",
      "parte midas completata\n",
      "\n",
      "0: 384x640 6 cars, 5.5ms\n",
      "Speed: 1.0ms preprocess, 5.5ms inference, 1.5ms postprocess per image at shape (1, 3, 384, 640)\n",
      "parte yolo completata\n",
      "parte generale completata\n",
      "parte midas completata\n",
      "\n",
      "0: 384x640 5 cars, 7.0ms\n",
      "Speed: 1.0ms preprocess, 7.0ms inference, 1.5ms postprocess per image at shape (1, 3, 384, 640)\n",
      "parte yolo completata\n",
      "parte generale completata\n",
      "parte midas completata\n",
      "\n",
      "0: 384x640 5 cars, 5.5ms\n",
      "Speed: 1.5ms preprocess, 5.5ms inference, 1.5ms postprocess per image at shape (1, 3, 384, 640)\n",
      "parte yolo completata\n",
      "parte generale completata\n",
      "parte midas completata\n",
      "\n",
      "0: 384x640 4 cars, 5.5ms\n",
      "Speed: 1.0ms preprocess, 5.5ms inference, 1.5ms postprocess per image at shape (1, 3, 384, 640)\n",
      "parte yolo completata\n",
      "parte generale completata\n",
      "parte midas completata\n",
      "\n",
      "0: 384x640 5 cars, 5.5ms\n",
      "Speed: 1.0ms preprocess, 5.5ms inference, 1.5ms postprocess per image at shape (1, 3, 384, 640)\n",
      "parte yolo completata\n",
      "parte generale completata\n",
      "parte midas completata\n",
      "\n",
      "0: 384x640 5 cars, 5.5ms\n",
      "Speed: 1.0ms preprocess, 5.5ms inference, 1.5ms postprocess per image at shape (1, 3, 384, 640)\n",
      "parte yolo completata\n",
      "parte generale completata\n",
      "parte midas completata\n",
      "\n",
      "0: 384x640 5 cars, 6.5ms\n",
      "Speed: 1.0ms preprocess, 6.5ms inference, 1.5ms postprocess per image at shape (1, 3, 384, 640)\n",
      "parte yolo completata\n",
      "parte generale completata\n",
      "parte midas completata\n",
      "\n",
      "0: 384x640 5 cars, 5.5ms\n",
      "Speed: 1.5ms preprocess, 5.5ms inference, 1.5ms postprocess per image at shape (1, 3, 384, 640)\n",
      "parte yolo completata\n",
      "parte generale completata\n",
      "parte midas completata\n",
      "\n",
      "0: 384x640 4 cars, 6.0ms\n",
      "Speed: 1.0ms preprocess, 6.0ms inference, 1.5ms postprocess per image at shape (1, 3, 384, 640)\n",
      "parte yolo completata\n",
      "parte generale completata\n",
      "parte midas completata\n",
      "\n",
      "0: 384x640 4 cars, 1 traffic light, 5.5ms\n",
      "Speed: 1.5ms preprocess, 5.5ms inference, 1.5ms postprocess per image at shape (1, 3, 384, 640)\n",
      "parte yolo completata\n",
      "parte generale completata\n",
      "parte midas completata\n",
      "\n",
      "0: 384x640 2 cars, 5.5ms\n",
      "Speed: 1.5ms preprocess, 5.5ms inference, 1.5ms postprocess per image at shape (1, 3, 384, 640)\n",
      "parte yolo completata\n",
      "parte generale completata\n",
      "parte midas completata\n",
      "\n",
      "0: 384x640 2 cars, 5.5ms\n",
      "Speed: 1.5ms preprocess, 5.5ms inference, 1.0ms postprocess per image at shape (1, 3, 384, 640)\n",
      "parte yolo completata\n",
      "parte generale completata\n",
      "parte midas completata\n",
      "\n",
      "0: 384x640 4 cars, 5.5ms\n",
      "Speed: 1.5ms preprocess, 5.5ms inference, 1.0ms postprocess per image at shape (1, 3, 384, 640)\n",
      "parte yolo completata\n",
      "parte generale completata\n",
      "parte midas completata\n",
      "\n",
      "0: 384x640 2 cars, 5.5ms\n",
      "Speed: 1.0ms preprocess, 5.5ms inference, 1.5ms postprocess per image at shape (1, 3, 384, 640)\n",
      "parte yolo completata\n",
      "parte generale completata\n",
      "parte midas completata\n",
      "\n",
      "0: 384x640 2 cars, 6.0ms\n",
      "Speed: 1.0ms preprocess, 6.0ms inference, 1.0ms postprocess per image at shape (1, 3, 384, 640)\n",
      "parte yolo completata\n",
      "parte generale completata\n",
      "parte midas completata\n",
      "\n",
      "0: 384x640 2 cars, 6.0ms\n",
      "Speed: 1.0ms preprocess, 6.0ms inference, 1.0ms postprocess per image at shape (1, 3, 384, 640)\n",
      "parte yolo completata\n",
      "parte generale completata\n",
      "parte midas completata\n",
      "\n",
      "0: 384x640 2 cars, 5.5ms\n",
      "Speed: 1.0ms preprocess, 5.5ms inference, 1.5ms postprocess per image at shape (1, 3, 384, 640)\n",
      "parte yolo completata\n",
      "parte generale completata\n",
      "parte midas completata\n",
      "\n",
      "0: 384x640 2 cars, 5.5ms\n",
      "Speed: 1.0ms preprocess, 5.5ms inference, 1.5ms postprocess per image at shape (1, 3, 384, 640)\n",
      "parte yolo completata\n",
      "parte generale completata\n",
      "parte midas completata\n",
      "\n",
      "0: 384x640 2 cars, 6.0ms\n",
      "Speed: 1.0ms preprocess, 6.0ms inference, 1.0ms postprocess per image at shape (1, 3, 384, 640)\n",
      "parte yolo completata\n",
      "parte generale completata\n",
      "parte midas completata\n",
      "\n",
      "0: 384x640 2 cars, 5.5ms\n",
      "Speed: 1.0ms preprocess, 5.5ms inference, 1.0ms postprocess per image at shape (1, 3, 384, 640)\n",
      "parte yolo completata\n",
      "parte generale completata\n",
      "parte midas completata\n",
      "\n",
      "0: 384x640 2 cars, 6.0ms\n",
      "Speed: 1.0ms preprocess, 6.0ms inference, 1.0ms postprocess per image at shape (1, 3, 384, 640)\n",
      "parte yolo completata\n",
      "parte generale completata\n",
      "parte midas completata\n",
      "\n",
      "0: 384x640 3 cars, 5.5ms\n",
      "Speed: 1.0ms preprocess, 5.5ms inference, 2.0ms postprocess per image at shape (1, 3, 384, 640)\n",
      "parte yolo completata\n",
      "parte generale completata\n",
      "parte midas completata\n",
      "\n",
      "0: 384x640 2 cars, 2 traffic lights, 6.0ms\n",
      "Speed: 1.0ms preprocess, 6.0ms inference, 1.0ms postprocess per image at shape (1, 3, 384, 640)\n",
      "parte yolo completata\n",
      "parte generale completata\n",
      "parte midas completata\n",
      "\n",
      "0: 384x640 2 cars, 1 traffic light, 5.5ms\n",
      "Speed: 1.0ms preprocess, 5.5ms inference, 1.5ms postprocess per image at shape (1, 3, 384, 640)\n",
      "parte yolo completata\n",
      "parte generale completata\n",
      "parte midas completata\n",
      "\n",
      "0: 384x640 3 cars, 5.5ms\n",
      "Speed: 1.0ms preprocess, 5.5ms inference, 1.5ms postprocess per image at shape (1, 3, 384, 640)\n",
      "parte yolo completata\n",
      "parte generale completata\n",
      "parte midas completata\n",
      "\n",
      "0: 384x640 3 cars, 1 traffic light, 6.5ms\n",
      "Speed: 1.0ms preprocess, 6.5ms inference, 1.5ms postprocess per image at shape (1, 3, 384, 640)\n",
      "parte yolo completata\n",
      "parte generale completata\n",
      "parte midas completata\n",
      "\n",
      "0: 384x640 1 car, 4 traffic lights, 6.0ms\n",
      "Speed: 1.0ms preprocess, 6.0ms inference, 1.0ms postprocess per image at shape (1, 3, 384, 640)\n",
      "parte yolo completata\n",
      "parte generale completata\n",
      "parte midas completata\n",
      "\n",
      "0: 384x640 2 cars, 1 traffic light, 5.5ms\n",
      "Speed: 1.5ms preprocess, 5.5ms inference, 1.5ms postprocess per image at shape (1, 3, 384, 640)\n",
      "parte yolo completata\n",
      "parte generale completata\n",
      "parte midas completata\n",
      "\n",
      "0: 384x640 1 car, 1 traffic light, 6.0ms\n",
      "Speed: 1.5ms preprocess, 6.0ms inference, 1.0ms postprocess per image at shape (1, 3, 384, 640)\n",
      "parte yolo completata\n",
      "parte generale completata\n",
      "parte midas completata\n",
      "\n",
      "0: 384x640 2 cars, 1 traffic light, 6.0ms\n",
      "Speed: 1.0ms preprocess, 6.0ms inference, 1.0ms postprocess per image at shape (1, 3, 384, 640)\n",
      "parte yolo completata\n",
      "parte generale completata\n",
      "parte midas completata\n",
      "\n",
      "0: 384x640 2 cars, 2 traffic lights, 6.0ms\n",
      "Speed: 1.0ms preprocess, 6.0ms inference, 1.5ms postprocess per image at shape (1, 3, 384, 640)\n",
      "parte yolo completata\n",
      "parte generale completata\n",
      "parte midas completata\n",
      "\n",
      "0: 384x640 2 cars, 4 traffic lights, 6.5ms\n",
      "Speed: 1.0ms preprocess, 6.5ms inference, 1.5ms postprocess per image at shape (1, 3, 384, 640)\n",
      "parte yolo completata\n",
      "parte generale completata\n",
      "parte midas completata\n",
      "\n",
      "0: 384x640 3 cars, 4 traffic lights, 6.5ms\n",
      "Speed: 1.0ms preprocess, 6.5ms inference, 1.0ms postprocess per image at shape (1, 3, 384, 640)\n",
      "parte yolo completata\n",
      "parte generale completata\n",
      "parte midas completata\n",
      "\n",
      "0: 384x640 2 cars, 5 traffic lights, 5.5ms\n",
      "Speed: 1.0ms preprocess, 5.5ms inference, 1.5ms postprocess per image at shape (1, 3, 384, 640)\n",
      "parte yolo completata\n",
      "parte generale completata\n",
      "parte midas completata\n",
      "\n",
      "0: 384x640 2 cars, 6 traffic lights, 5.5ms\n",
      "Speed: 1.0ms preprocess, 5.5ms inference, 1.5ms postprocess per image at shape (1, 3, 384, 640)\n",
      "parte yolo completata\n",
      "parte generale completata\n",
      "parte midas completata\n",
      "\n",
      "0: 384x640 2 cars, 6 traffic lights, 6.0ms\n",
      "Speed: 1.0ms preprocess, 6.0ms inference, 1.5ms postprocess per image at shape (1, 3, 384, 640)\n",
      "parte yolo completata\n",
      "parte generale completata\n",
      "parte midas completata\n",
      "\n",
      "0: 384x640 2 cars, 6 traffic lights, 6.5ms\n",
      "Speed: 1.0ms preprocess, 6.5ms inference, 1.0ms postprocess per image at shape (1, 3, 384, 640)\n",
      "parte yolo completata\n",
      "parte generale completata\n",
      "parte midas completata\n",
      "\n",
      "0: 384x640 2 cars, 8 traffic lights, 6.0ms\n",
      "Speed: 1.0ms preprocess, 6.0ms inference, 1.5ms postprocess per image at shape (1, 3, 384, 640)\n",
      "parte yolo completata\n",
      "parte generale completata\n",
      "parte midas completata\n",
      "\n",
      "0: 384x640 2 cars, 6 traffic lights, 5.5ms\n",
      "Speed: 1.0ms preprocess, 5.5ms inference, 1.5ms postprocess per image at shape (1, 3, 384, 640)\n",
      "parte yolo completata\n",
      "parte generale completata\n",
      "parte midas completata\n",
      "\n",
      "0: 384x640 5 cars, 5 traffic lights, 5.5ms\n",
      "Speed: 1.5ms preprocess, 5.5ms inference, 1.5ms postprocess per image at shape (1, 3, 384, 640)\n",
      "parte yolo completata\n",
      "parte generale completata\n",
      "parte midas completata\n",
      "\n",
      "0: 384x640 3 cars, 9 traffic lights, 5.5ms\n",
      "Speed: 1.0ms preprocess, 5.5ms inference, 1.5ms postprocess per image at shape (1, 3, 384, 640)\n",
      "parte yolo completata\n",
      "parte generale completata\n",
      "parte midas completata\n",
      "\n",
      "0: 384x640 2 cars, 8 traffic lights, 6.0ms\n",
      "Speed: 1.0ms preprocess, 6.0ms inference, 1.0ms postprocess per image at shape (1, 3, 384, 640)\n",
      "parte yolo completata\n",
      "parte generale completata\n",
      "parte midas completata\n",
      "\n",
      "0: 384x640 3 cars, 6 traffic lights, 5.8ms\n",
      "Speed: 1.2ms preprocess, 5.8ms inference, 1.0ms postprocess per image at shape (1, 3, 384, 640)\n",
      "parte yolo completata\n",
      "parte generale completata\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Edoardo\\AppData\\Local\\Temp\\ipykernel_8900\\144017356.py:20: RuntimeWarning: divide by zero encountered in scalar divide\n",
      "  real_depth = coefficiente / depth\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "parte midas completata\n",
      "\n",
      "0: 384x640 3 cars, 1 truck, 6 traffic lights, 6.5ms\n",
      "Speed: 1.5ms preprocess, 6.5ms inference, 1.5ms postprocess per image at shape (1, 3, 384, 640)\n",
      "parte yolo completata\n",
      "parte generale completata\n",
      "parte midas completata\n",
      "\n",
      "0: 384x640 4 cars, 7 traffic lights, 6.0ms\n",
      "Speed: 1.0ms preprocess, 6.0ms inference, 1.0ms postprocess per image at shape (1, 3, 384, 640)\n",
      "parte yolo completata\n",
      "parte generale completata\n",
      "parte midas completata\n",
      "\n",
      "0: 384x640 4 cars, 6 traffic lights, 5.5ms\n",
      "Speed: 0.5ms preprocess, 5.5ms inference, 1.5ms postprocess per image at shape (1, 3, 384, 640)\n",
      "parte yolo completata\n",
      "parte generale completata\n",
      "parte midas completata\n",
      "\n",
      "0: 384x640 3 cars, 5 traffic lights, 5.5ms\n",
      "Speed: 1.5ms preprocess, 5.5ms inference, 1.5ms postprocess per image at shape (1, 3, 384, 640)\n",
      "parte yolo completata\n",
      "parte generale completata\n"
     ]
    }
   ],
   "source": [
    "video_path = \"./references/video/cardrive.mp4\"\n",
    "cap = cv2.VideoCapture(video_path)\n",
    "\n",
    "# Ottieni il framerate (FPS) del video\n",
    "fps = int(cap.get(cv2.CAP_PROP_FPS))\n",
    "frame_to_elaborate = int(fps / 4)\n",
    "frame_time = int(1000/fps)\n",
    "frame_count = 0\n",
    "coefficiente = 100\n",
    "\n",
    "while cap.isOpened():\n",
    "\n",
    "    # Leggi il frame corrente\n",
    "    ret, frame = cap.read()\n",
    "    if not ret:\n",
    "        break  # Esci se il video Ã¨ terminato\n",
    "    # Processa solo 1 frame ogni secondo\n",
    "    if frame_count % frame_to_elaborate == 0:\n",
    "        #parte MiDaS\n",
    "        midas_output = midas_elaboration(frame)\n",
    "        print(\"parte midas completata\")\n",
    "        #parte YOLO\n",
    "        yolo_output = yolo_elaboration(frame)\n",
    "        print(\"parte yolo completata\")\n",
    "        #fu-sio-nee\n",
    "        general_output = elaboration(midas_output, yolo_output, coefficiente, 4)\n",
    "        print(\"parte generale completata\")\n",
    "        \"\"\"\n",
    "        debug\n",
    "        print(general_output)\n",
    "        \"\"\"\n",
    "        #mappa 3D dell'ambiente circostante\n",
    "        threedMapConversion(general_output, 0)\n",
    "        \n",
    "    frame_count += 1\n",
    "\n",
    "    if cv2.waitKey(15) & 0xFF == ord('q'):\n",
    "        break\n",
    "\n",
    "cap.release()\n",
    "cv2.destroyAllWindows()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import cv2\n",
    "import os\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import yaml\n",
    "import random"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "class DataLoader(object):\n",
    "    main_path = \"dataset/2011_09_26_drive_0035_sync\"\n",
    "    def __init__(self):\n",
    "        self.left_camera_images = sorted(os.listdir(\"dataset/2011_09_26_drive_0035_sync/image_02/data\"))\n",
    "        self.right_camera_images = sorted(os.listdir(\"dataset/2011_09_26_drive_0035_sync/image_03/data\"))\n",
    "        print(self.left_camera_images)\n",
    "        self.frames = len(self.left_camera_images)\n",
    "\n",
    "        calibration = pd.read_csv(\"dataset/2011_09_26_drive_0035_sync/calib_cam_to_cam.txt\", delimiter=' ', header=None, index_col=0)\n",
    "        #projection matrix\n",
    "        self.P0 = np.array(calibration.loc['P_rect_02:'], dtype=np.float32).reshape((3,4))\n",
    "        self.P1 = np.array(calibration.loc['P_rect_03:'], dtype=np.float32).reshape((3,4))\n",
    "\n",
    "        #image loader\n",
    "        self.left_images = []\n",
    "        self.right_images = []\n",
    "\n",
    "        for i, left in enumerate(self.left_camera_images):\n",
    "            right = self.right_camera_images[i]\n",
    "            self.left_images.append(cv2.imread('dataset/2011_09_26_drive_0035_sync/image_02/data/' + left))\n",
    "            self.right_images.append(cv2.imread('dataset/2011_09_26_drive_0035_sync/image_03/data/' + right))\n",
    "        \n",
    "        self.first_image_left = self.left_images[0]\n",
    "        self.first_image_right = self.right_images[0]\n",
    "        self.second_image_left = self.left_images[1]\n",
    "        self.second_image_right = self.right_images[1]\n",
    "\n",
    "        self.image_height = self.left_images[0].shape[0]\n",
    "        self.image_width = self.left_images[0].shape[1]\n",
    "\n",
    "    def reset_frames(self):\n",
    "        self.left_images = (cv2.imread('dataset/2011_09_26_drive_0035_sync/image_02/data/' + left, 0) for left in self.left_camera_images)\n",
    "        self.right_images = (cv2.imread('dataset/2011_09_26_drive_0035_sync/image_03/data/' + right, 0) for right in self.right_camera_images)\n",
    "        pass\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(\"./initial_config.yaml\", \"r\") as stream:\n",
    "    try:\n",
    "        config = yaml.safe_load(stream)\n",
    "    except yaml.YAMLError as error:\n",
    "        print(error)\n",
    "\n",
    "rgb_value = config['parameters']['rgb']\n",
    "rectified_value = config['parameters']['rectified']\n",
    "detector_name = config['parameters']['detector']\n",
    "max_depth_value = config['parameters']['max_depth']\n",
    "\n",
    "##stereo depth estimation\n",
    "\n",
    "def disparity_mapping(left_image, right_image, rgb=rgb_value):\n",
    "    if rgb:\n",
    "        num_channels = 3\n",
    "    else:\n",
    "        num_channels = 1\n",
    "\n",
    "    num_disparities = 6*16\n",
    "    block_size = 7\n",
    "\n",
    "    matcher = cv2.StereoSGBM_create(numDisparities = num_disparities, \n",
    "                                    minDisparity = 0, \n",
    "                                    blockSize = block_size, \n",
    "                                    P1=8 * num_channels * block_size ** 2,\n",
    "                                    P2=32 * num_channels * block_size ** 2,\n",
    "                                    mode = cv2.STEREO_SGBM_MODE_SGBM_3WAY)\n",
    "    if rgb:\n",
    "        left_image = cv2.cvtColor(left_image, cv2.COLOR_BGR2GRAY)\n",
    "        right_image = cv2.cvtColor(right_image, cv2.COLOR_BGR2GRAY)\n",
    "    \n",
    "    left_image_disparity_map = matcher.compute(left_image, right_image).astype(np.float32)/16\n",
    "\n",
    "    return left_image_disparity_map\n",
    "\n",
    "def decomposition(p):\n",
    "\n",
    "    intrinsic_matrix, rotation_matrix, translation_vector, _, _, _, _ = cv2.decomposeProjectionMatrix(p)\n",
    "\n",
    "    translation_vector = (translation_vector / translation_vector[3])[:3]\n",
    "\n",
    "    return intrinsic_matrix, rotation_matrix, translation_vector\n",
    "\n",
    "def depth_mapping(left_disparity_map, left_intrinsic, left_translation, right_translation, rectified=rectified_value):\n",
    "\n",
    "    focal_length = left_intrinsic[0][0]\n",
    "\n",
    "    if rectified:\n",
    "        baseline = right_translation[0] - left_translation[0]\n",
    "    else:\n",
    "        baseline = left_translation[0] - right_translation[0]\n",
    "\n",
    "    left_disparity_map[left_disparity_map == 0.0] = 0.1\n",
    "    left_disparity_map[left_disparity_map == -1.0] = 0.1\n",
    "\n",
    "    depth_map = np.ones(left_disparity_map.shape)\n",
    "    depth_map = (focal_length * baseline) / left_disparity_map\n",
    "\n",
    "    return depth_map\n",
    "\n",
    "def stereo_depth(left_image, right_image, P0, P1, rgb=rgb_value):\n",
    "\n",
    "    disp_map = disparity_mapping(left_image, right_image, rgb=rgb)\n",
    "\n",
    "    l_intrinsic, l_rotation, l_translation = decomposition(P0)\n",
    "    r_intrinsic, r_rotation, r_translation = decomposition(P1)\n",
    "\n",
    "    depth = depth_mapping(disp_map, l_intrinsic, l_translation, r_translation)\n",
    "\n",
    "    return depth\n",
    "\n",
    "##stereo depth estimation\n",
    "\n",
    "\n",
    "##feature extraction and matching\n",
    "\n",
    "def feature_extractor(image, detector=detector_name,mask=None):\n",
    "\n",
    "    if detector == 'sift':\n",
    "        create_detector = cv2.SIFT_create()\n",
    "    elif detector == 'orb':\n",
    "        create_detector = cv2.ORB_create()\n",
    "\n",
    "    keypoints, descriptors = create_detector.detectAndCompute(image, mask)\n",
    "\n",
    "    return keypoints, descriptors\n",
    "\n",
    "def feature_matching(first_descriptor, second_descriptor, detector=detector_name, k=2, distance_threshold=1.0):\n",
    "\n",
    "    if detector == 'sift':\n",
    "        feature_matcher = cv2.BFMatcher_create(cv2.NORM_L2, crossCheck = False)\n",
    "    elif detector == 'orb':\n",
    "        feature_matcher = cv2.BFMatcher_create(cv2.NORM_L2, crossCheck = False)\n",
    "    matches = feature_matcher.knnMatch(first_descriptor, second_descriptor, k=k)\n",
    "    filtered_matches = []\n",
    "    for match1, match2 in matches:\n",
    "        if match1.distance <= distance_threshold * match2.distance:\n",
    "            filtered_matches.append(match1)\n",
    "\n",
    "    return filtered_matches\n",
    "\n",
    "def visualize_matches(first_image, second_image, keypoint_one, keypoint_two, matches):\n",
    "    show_matches = cv2.drawMatches(first_image, keypoint_one, second_image, keypoint_two, matches, None, flags = 2)\n",
    "    plt.figure(figsize=(15, 5), dpi=100)\n",
    "    plt.imshow(show_matches)\n",
    "    plt.show()\n",
    "\n",
    "##feature extraction and matching\n",
    "\n",
    "##motion estimation\n",
    "\n",
    "def motion_estimation(matches, firstImage_keypoints, secondImage_keypoints, intrinsic_matrix, depth, max_depth=max_depth_value):\n",
    "    \"\"\"\n",
    "    Estimating motion of the left camera from sequential imgaes \n",
    "\n",
    "    \"\"\"\n",
    "    rotation_matrix = np.eye(3)\n",
    "    translation_vector = np.zeros((3, 1))\n",
    "\n",
    "    # Only considering keypoints that are matched for two sequential frames\n",
    "    image1_points = np.float32(\n",
    "        [firstImage_keypoints[m.queryIdx].pt for m in matches])\n",
    "    image2_points = np.float32(\n",
    "        [secondImage_keypoints[m.trainIdx].pt for m in matches])\n",
    "\n",
    "    cx = intrinsic_matrix[0, 2]\n",
    "    cy = intrinsic_matrix[1, 2]\n",
    "    fx = intrinsic_matrix[0, 0]\n",
    "    fy = intrinsic_matrix[1, 1]\n",
    "\n",
    "    points_3D = np.zeros((0, 3))\n",
    "    outliers = []\n",
    "\n",
    "    # Extract depth information to build 3D positions\n",
    "    for indices, (u, v) in enumerate(image1_points):\n",
    "        z = depth[int(v), int(u)]\n",
    "\n",
    "        # We will not consider depth greater than max_depth\n",
    "        if z > max_depth:\n",
    "            outliers.append(indices)\n",
    "            continue\n",
    "\n",
    "        # Using z we can find the x,y points in 3D coordinate using the formula\n",
    "        x = z*(u-cx)/fx\n",
    "        y = z*(v-cy)/fy\n",
    "\n",
    "        # Stacking all the 3D (x,y,z) points\n",
    "        points_3D = np.vstack([points_3D, np.array([x, y, z])])\n",
    "\n",
    "    # Deleting the false depth points\n",
    "    image1_points = np.delete(image1_points, outliers, 0)\n",
    "    image2_points = np.delete(image2_points, outliers, 0)\n",
    "\n",
    "    # Apply Ransac Algorithm to remove outliers\n",
    "    _, rvec, translation_vector, _ = cv2.solvePnPRansac(\n",
    "        points_3D, image2_points, intrinsic_matrix, None)\n",
    "\n",
    "    rotation_matrix = cv2.Rodrigues(rvec)[0]\n",
    "\n",
    "    return rotation_matrix, translation_vector, image1_points, image2_points\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "yolo_model = YOLO('yolov8n.pt') "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "definizione della funzione yolo_elaboration da inserire nel ciclo della gestione dei frame del video"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def yolo_elaboration(frame):\n",
    "    detection_info = []\n",
    "    detection_id = []\n",
    "    #carica l'immagine\n",
    "    #img = cv2.imread(frame)\n",
    "\n",
    "    #esegui il rilevamento con yolov8\n",
    "    yolo_results = yolo_model.track(frame, persist=True)\n",
    "\n",
    "    #ottieni i risultati del rilevamento (results[0] indica le informazioni ottenute dall'unica immagine in input, se ci fossero piu di una immagine allora sarebbe results[i] con i = 1...n)\n",
    "    detection = yolo_results[0].boxes.xyxy #coordinate della bounding box\n",
    "    confidences = yolo_results[0].boxes.conf #confidenza della rilevazione\n",
    "    class_ids = yolo_results[0].boxes.cls #ID delle classi di appartenenza dell'oggetto\n",
    "    obj_ids = yolo_results[0].boxes.id #ID dell'oggetto\n",
    "\n",
    "    #visualizza i risultati\n",
    "    for i, (box, conf, cls_id, obj_id) in enumerate(zip(detection, confidences, class_ids, obj_ids)):\n",
    "        x1, y1, x2, y2 = map(int, box) #estrai le coordinate della bounding box\n",
    "\n",
    "        label = f\"{yolo_model.names[int(cls_id)]} ({conf:.2f})\" #viene creata un'etichetta contenente il nome della classe (poiche class_ids e' un intero che grazie a model.names[cls_id] viene convertito nella corrispondente classe) e la confidenza\n",
    "\n",
    "        detection_info.append({\n",
    "            #'frame': capture.get(cv2.CAP_PROP_POS_FRAMES),\n",
    "            'id': obj_id,\n",
    "            'label': label,\n",
    "            'confidence': conf,\n",
    "            'class': cls_id,\n",
    "            'x1': x1,\n",
    "            'x2': x2,\n",
    "            'y1': y1,\n",
    "            'y2': y2\n",
    "        })\n",
    "        detection_id.append(obj_id)\n",
    "    #annotated_frame = yolo_results[0].plot()\n",
    "    #cv2.waitKey(0)\n",
    "    #cv2.imshow(\"si\", annotated_frame)\n",
    "    #cv2.destroyAllWindows()\n",
    "    return detection_info, detection_id"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "combinazione dei risultati delle due funzioni precedenti per calcolare la distanza stimata degli oggetti rilevati nel frame"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def elaboration(depth_map, yolo, old_detection, old_detection_id, intrinsic_matrix, transformation_matrix):\n",
    "    detection_info = yolo\n",
    "    cx = intrinsic_matrix[0, 2]\n",
    "    cy = intrinsic_matrix[1, 2]\n",
    "    fx = intrinsic_matrix[0, 0]\n",
    "    fy = intrinsic_matrix[1, 1]\n",
    "    for object in detection_info:\n",
    "        obj_id = object['id'] \n",
    "        #se l'oggetto rilevato e' stato gia scansionato precedentemente, ne miglioro la posizione aggiungendo le coordinate dei pixel della scansione nuova\n",
    "        if obj_id in old_detection_id:\n",
    "            object_data = []\n",
    "            for obj in old_detection:\n",
    "                if obj['id'] == obj_id:\n",
    "                    object_data = obj['3dcoord']\n",
    "                    break\n",
    "            \n",
    "            label = object['label']\n",
    "            confidence = object['confidence']\n",
    "            x1 = object['x1']\n",
    "            x2 = object['x2']\n",
    "            y1 = object['y1']\n",
    "            y2 = object['y2']\n",
    "            if confidence < 0.50:\n",
    "                continue\n",
    "            for y in range(y1, y2):\n",
    "                for x in range(x1, x2):\n",
    "                    #campiono il 20% dei pixel all'interno della bounding box\n",
    "                    if random.random() > 0.2:\n",
    "                        continue\n",
    "                    depth = depth_map[y,x]\n",
    "                    real_x = ((x - cx) * depth)/fx\n",
    "                    real_y = ((y - cy) * depth)/fy\n",
    "                    obj_coords = np.array([real_x, real_y, depth, 1])\n",
    "                    world_obj_coords = transformation_matrix.dot(obj_coords)\n",
    "                    object_data.append(world_obj_coords)\n",
    "            for i, obj in enumerate(old_detection):\n",
    "                if obj['id'] == obj_id:\n",
    "                    old_detection[i] = {'id': obj_id,\n",
    "                                        'label': label,\n",
    "                                        'confidence': confidence,\n",
    "                                        '3dcoord': object_data}\n",
    "            continue\n",
    "        #se l'oggetto scansionato non era stato scansionato precedentemente, aggiungo un nuovo elemento nella lista old_detection che contiene varie informazioni tra cui l'id e le coordinate dei pixel del nuovo oggetto\n",
    "        label = object['label']\n",
    "        confidence = object['confidence']\n",
    "        x1 = object['x1']\n",
    "        x2 = object['x2']\n",
    "        y1 = object['y1']\n",
    "        y2 = object['y2']\n",
    "        if confidence < 0.50:\n",
    "            continue\n",
    "        object_data = []\n",
    "        for y in range(y1, y2):\n",
    "            for x in range(x1, x2):\n",
    "                #campiono il 20% dei pixel all'interno della bounding box\n",
    "                if random.random() > 0.2:\n",
    "                    continue\n",
    "                depth = depth_map[y,x]\n",
    "                real_x = ((x - cx) * depth)/fx\n",
    "                real_y = ((y - cy) * depth)/fy\n",
    "                obj_coords = np.array([real_x, real_y, depth, 1])\n",
    "                world_obj_coords = transformation_matrix.dot(obj_coords)\n",
    "                object_data.append(world_obj_coords)\n",
    "        old_detection.append({\n",
    "            'id' : obj_id,\n",
    "            'label' : label,\n",
    "            'confidence' : confidence,\n",
    "            '3dcoord' : object_data\n",
    "        })\n",
    "    return old_detection\n",
    "        \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def threedMapConversion(detection_info):\n",
    "    points = []\n",
    "    for object in detection_info:\n",
    "        threedcoords = [coord[:3] for coord in object['3dcoord']]\n",
    "        points.extend(threedcoords)\n",
    "    points = np.array(points)\n",
    "    pcd = o3d.geometry.PointCloud()\n",
    "    pcd.points = o3d.utility.Vector3dVector(points)\n",
    "    o3d.visualization.draw_geometries([pcd])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(\"./initial_config.yaml\", \"r\") as stream:\n",
    "    try:\n",
    "        config = yaml.safe_load(stream)\n",
    "    except yaml.YAMLError as error:\n",
    "        print(error)\n",
    "\n",
    "detector_name = config['parameters']['detector']\n",
    "subset = config['parameters']['subset']\n",
    "threshold = config['parameters']['distance_threshold']\n",
    "\n",
    "def odometry(data_handler, detector = detector_name, mask = None, subset = subset, plot = True):\n",
    "\n",
    "    if subset is not None:\n",
    "        num_frames = subset\n",
    "    else:\n",
    "        num_frames = data_handler.frames\n",
    "\n",
    "    # Create a homogeneous matrix\n",
    "    homo_matrix = np.eye(4)\n",
    "    trajectory = np.zeros((num_frames, 3, 4))\n",
    "    trajectory[0] = homo_matrix[:3, :]\n",
    "\n",
    "    # From projection matrix retrieve the left camera's intrinsic matrix\n",
    "    left_intrinsic_matrix, _, _ = decomposition(data_handler.P0)\n",
    "\n",
    "    #yolo\n",
    "    detection = []\n",
    "    detection_id = []\n",
    "\n",
    "    # Loop to iterate all the frames\n",
    "    for i in range(num_frames - 1):\n",
    "\n",
    "        image_left = data_handler.left_images[i]\n",
    "        image_right = data_handler.right_images[i]\n",
    "        next_image = data_handler.left_images[i + 1]\n",
    "\n",
    "        # Estimating the depth of an image\n",
    "        depth = stereo_depth(image_left, image_right, P0=data_handler.P0, P1=data_handler.P1)\n",
    "        \n",
    "        # Keypoints and Descriptors of two sequential images of the left camera\n",
    "        keypoint_left_first, descriptor_left_first = feature_extractor(image_left, detector, mask)\n",
    "        keypoint_left_next, descriptor_left_next = feature_extractor(next_image, detector, mask)\n",
    "\n",
    "        # Use feature (e.g. SIFT or ORB) detector to match features\n",
    "        matches = feature_matching(descriptor_left_first, descriptor_left_next, detector=detector, distance_threshold=threshold)\n",
    "\n",
    "        # Estimate motion between sequential images of the left camera\n",
    "        rotation_matrix, translation_vector, _, _ = motion_estimation(matches, keypoint_left_first, keypoint_left_next, left_intrinsic_matrix, depth)\n",
    "\n",
    "        # Initialise a homogeneous matrix (4X4)\n",
    "        Transformation_matrix = np.eye(4)\n",
    "\n",
    "        # Build the Transformation matrix using rotation matrix and translation vector from motion estimation function\n",
    "        Transformation_matrix[:3, :3] = rotation_matrix\n",
    "        Transformation_matrix[:3, 3] = translation_vector.T\n",
    "\n",
    "        # Transformation wrt. world coordinate system\n",
    "        homo_matrix = homo_matrix.dot(np.linalg.inv(Transformation_matrix))\n",
    "\n",
    "        # Append the pose of camera in the trajectory array\n",
    "        trajectory[i+1, :, :] = homo_matrix[:3, :]\n",
    "\n",
    "        #yolo\n",
    "        new_detection, new_detection_id = yolo_elaboration(image_left)\n",
    "\n",
    "        #elaboration\n",
    "        detection = elaboration(depth, new_detection, detection, detection_id, left_intrinsic_matrix, homo_matrix)\n",
    "\n",
    "        detection_id.extend(new_detection_id)\n",
    "        if i % 10 == 0:\n",
    "            print(f'{i} frames have been computed')\n",
    "\n",
    "        if i == 6 - 2:\n",
    "            print('All frames have been computed')\n",
    "            break\n",
    "    threedMapConversion(detection)\n",
    "    return trajectory\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['0000000000.png', '0000000001.png', '0000000002.png', '0000000003.png', '0000000004.png', '0000000005.png', '0000000006.png', '0000000007.png', '0000000008.png', '0000000009.png', '0000000010.png', '0000000011.png', '0000000012.png', '0000000013.png', '0000000014.png', '0000000015.png', '0000000016.png', '0000000017.png', '0000000018.png', '0000000019.png', '0000000020.png', '0000000021.png', '0000000022.png', '0000000023.png', '0000000024.png', '0000000025.png', '0000000026.png', '0000000027.png', '0000000028.png', '0000000029.png', '0000000030.png', '0000000031.png', '0000000032.png', '0000000033.png', '0000000034.png', '0000000035.png', '0000000036.png', '0000000037.png', '0000000038.png', '0000000039.png', '0000000040.png', '0000000041.png', '0000000042.png', '0000000043.png', '0000000044.png', '0000000045.png', '0000000046.png', '0000000047.png', '0000000048.png', '0000000049.png', '0000000050.png', '0000000051.png', '0000000052.png', '0000000053.png', '0000000054.png', '0000000055.png', '0000000056.png', '0000000057.png', '0000000058.png', '0000000059.png', '0000000060.png', '0000000061.png', '0000000062.png', '0000000063.png', '0000000064.png', '0000000065.png', '0000000066.png', '0000000067.png', '0000000068.png', '0000000069.png', '0000000070.png', '0000000071.png', '0000000072.png', '0000000073.png', '0000000074.png', '0000000075.png', '0000000076.png', '0000000077.png', '0000000078.png', '0000000079.png', '0000000080.png', '0000000081.png', '0000000082.png', '0000000083.png', '0000000084.png', '0000000085.png', '0000000086.png', '0000000087.png', '0000000088.png', '0000000089.png', '0000000090.png', '0000000091.png', '0000000092.png', '0000000093.png', '0000000094.png', '0000000095.png', '0000000096.png', '0000000097.png', '0000000098.png', '0000000099.png', '0000000100.png', '0000000101.png', '0000000102.png', '0000000103.png', '0000000104.png', '0000000105.png', '0000000106.png', '0000000107.png', '0000000108.png', '0000000109.png', '0000000110.png', '0000000111.png', '0000000112.png', '0000000113.png', '0000000114.png', '0000000115.png', '0000000116.png', '0000000117.png', '0000000118.png', '0000000119.png', '0000000120.png', '0000000121.png', '0000000122.png', '0000000123.png', '0000000124.png', '0000000125.png', '0000000126.png', '0000000127.png', '0000000128.png', '0000000129.png', '0000000130.png']\n",
      "\n",
      "0: 224x640 6 cars, 44.5ms\n",
      "Speed: 1.0ms preprocess, 44.5ms inference, 10.0ms postprocess per image at shape (1, 3, 224, 640)\n",
      "0 frames have been computed\n",
      "\n",
      "0: 224x640 1 person, 7 cars, 33.0ms\n",
      "Speed: 1.5ms preprocess, 33.0ms inference, 4.0ms postprocess per image at shape (1, 3, 224, 640)\n",
      "\n",
      "0: 224x640 1 person, 6 cars, 31.5ms\n",
      "Speed: 1.5ms preprocess, 31.5ms inference, 6.0ms postprocess per image at shape (1, 3, 224, 640)\n",
      "\n",
      "0: 224x640 7 cars, 50.5ms\n",
      "Speed: 1.5ms preprocess, 50.5ms inference, 7.0ms postprocess per image at shape (1, 3, 224, 640)\n",
      "\n",
      "0: 224x640 7 cars, 22.5ms\n",
      "Speed: 2.0ms preprocess, 22.5ms inference, 8.5ms postprocess per image at shape (1, 3, 224, 640)\n",
      "All frames have been computed\n",
      "(136321, 3)\n"
     ]
    }
   ],
   "source": [
    "data_handler = DataLoader()\n",
    "\n",
    "trajectory = odometry(data_handler)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "myenv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
