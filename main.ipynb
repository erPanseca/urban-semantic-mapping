{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Sistema di Rilevamento e Mappatura 3D con Stereo Vision e YOLO\n",
    "\n",
    "## Obiettivo del Progetto\n",
    "Il progetto si propone di sviluppare un sistema avanzato per la rilevazione degli oggetti in una scena ripresa da una macchina in movimento. Il sistema è in grado di:\n",
    "\n",
    "1. **Calcolare la profondità**: Utilizzando un sistema stereo basato su SIFT e il feature matching.\n",
    "2. **Rilevare oggetti**: Applicando il modello YOLO per identificare e localizzare gli oggetti nella scena.\n",
    "3. **Determinare coordinate 3D**: Calcolando le posizioni tridimensionali degli oggetti rilevati.\n",
    "4. **Visualizzare una mappa 3D**: Utilizzando Open3D per rappresentare graficamente gli oggetti nella scena.\n",
    "\n",
    "---\n",
    "\n",
    "Per semplificare l'acquisizione dei dati, sono stati utilizzati i dataset kitti, che hanno fornito:\n",
    "- **Video stereo**: Catturati da una macchina in movimento.\n",
    "- **Parametri della telecamera**: Inclusi i dati di calibrazione necessari per il calcolo della profondità.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Calcolo della Profondità, Feature Matching e Stima del Movimento della Camera\n",
    "\n",
    "### 1. Calcolo della Profondità\n",
    "Per calcolare la profondità della scena, le immagini stereo vengono passate a una funzione wrapper chiamata `stereo_depth`. All'interno di questa funzione, per ogni iterazione, viene costruita una **disparity map** a partire da una coppia di immagini stereo.  \n",
    "La disparità è calcolata utilizzando la funzione `StereoSGBM` di OpenCV, un'implementazione dell'algoritmo di Hirschmüller. Una **disparity map** è mostrata di seguito:  \n",
    "\n",
    "![Immagine non disponibile](references/disparity_map.png)\n",
    "\n",
    "\n",
    "Una volta ottenuta la mappa di disparità, la profondità della scena viene calcolata utilizzando la formula:  \n",
    "\n",
    "### $z = \\frac{f \\times b}{d}$\n",
    "\n",
    "Dove:  \n",
    "- **z**  è la profondità,  \n",
    "- **f** è la lunghezza focale della camera,  \n",
    "- **b** è la baseline (distanza tra le due telecamere stereo),  \n",
    "- **d** è la disparità.  \n",
    "\n",
    "Questa formula ci consente di convertire la disparità in una **depth map**, che rappresenta le distanze degli oggetti nella scena.  \n",
    "\n",
    "![Immagine non disponibile](references/depth_map.png)\n",
    "\n",
    "---\n",
    "\n",
    "### 2. Estrazione di Feature e Matching\n",
    "Successivamente, le immagini catturate dalla camera sinistra vengono processate per estrarre **feature** (punti chiave e descrittori) utilizzando SIFT (Scale-Invariant Feature Transform) o ORB. Questi descrittori vengono passati a una funzione per calcolare i **matching** tra due frame consecutivi della sequenza video.  \n",
    "\n",
    "Per il matching viene utilizzata la funzione **Brute-Force Matching** con norma  $L2$  di OpenCV. Questa tecnica produce un gran numero di punti corrispondenti, ma non tutti sono corretti. Pertanto, viene applicata una soglia per filtrare i match deboli.  \n",
    "\n",
    "Dai nostri esperimenti, un valore di soglia compreso tra **0.25** e **0.45** fornisce un numero sufficiente di match accurati.\n",
    "\n",
    "Anche se ORB è più veloce, nei dataset KITTI utilizzati in questa pipeline, SIFT offre risultati migliori in termini di qualità del matching. Esempi:  \n",
    "- **SIFT** con soglia **0.25** fornisce un gran numero di match utili.  \n",
    "- **ORB** con soglia **0.75** fornisce molti meno punti corrispondenti.  \n",
    "\n",
    "![Immagine non disponibile](references/matches.png)\n",
    "\n",
    "---\n",
    "\n",
    "### 3. Calcolo delle Coordinate 3D\n",
    "Una volta ottenuto un numero sufficiente di punti corrispondenti (match), le coordinate 3D vengono calcolate utilizzando:  \n",
    "1. **Keypoints** rilevati.  \n",
    "2. **Matching** validi.  \n",
    "3. Parametri intrinseci della camera.\n",
    "\n",
    "La profondità **z** dei punti 3D viene recuperata dai match, mentre le coordinate **x** e **y** sono calcolate utilizzando le formule:  \n",
    "\n",
    "### $x = z \\cdot \\frac{(u - c_x)}{f_x}$\n",
    "  \n",
    "\n",
    "### $y = z \\cdot \\frac{(v - c_y)}{f_y}$\n",
    "  \n",
    "\n",
    "Dove:  \n",
    "- **$u,v$**: coordinate pixel del punto.  \n",
    "- **$f_x, f_y$**: distanza focale lungo gli assi **x** e **y**.  \n",
    "- **$c_x, c_y$**: centro ottico dell'immagine.  \n",
    "\n",
    "Le distanze focali e il centro ottico sono ottenuti dalla **matrice intrinseca** della camera.  \n",
    "\n",
    "---\n",
    "\n",
    "### 4. Stima del Movimento della Camera\n",
    "Dai punti 3D calcolati, viene stimato il movimento della camera tra frame consecutivi. Per migliorare l'accuratezza, viene applicato l'algoritmo **solvePnPRansac** per eliminare valori anomali dai punti 3D. Questo passaggio è essenziale per ottenere una stima affidabile della posizione e orientamento della camera.  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Jupyter environment detected. Enabling Open3D WebVisualizer.\n",
      "[Open3D INFO] WebRTC GUI backend enabled.\n",
      "[Open3D INFO] WebRTCWindowSystem: HTTP handshake server disabled.\n"
     ]
    }
   ],
   "source": [
    "import cv2\n",
    "import os\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import yaml\n",
    "import random\n",
    "from ultralytics import YOLO\n",
    "import open3d as o3d\n",
    "from scipy.ndimage import gaussian_filter\n",
    "from scipy.spatial import cKDTree\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Descrizione della Classe `DataLoader`\n",
    "\n",
    "La classe DataLoader è progettata per caricare e gestire le immagini stereo provenienti da un dataset, insieme ai parametri di calibrazione della telecamera.\n",
    "\n",
    "Quando viene inizializzata, la classe riceve come parametro un percorso (path) che punta alla directory contenente le immagini e i file di calibrazione.\n",
    "\n",
    "Le immagini della telecamera sinistra e destra vengono memorizzate in due liste separate, left_images e right_images, e la lunghezza di queste liste viene utilizzata per determinare il numero totale di coppie di immagini, rappresentato dalla variabile frames. \n",
    "\n",
    "Inoltre, la classe carica i parametri di calibrazione necessari per l'elaborazione delle immagini. Viene letto il file calib_cam_to_cam.txt, che contiene le matrici di proiezione per le telecamere sinistra e destra. Le matrici di calibrazione, P0 e P1, vengono quindi salvate come variabili della classe per essere utilizzate in seguito nel calcolo della profondità.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "class DataLoader(object):\n",
    "    def __init__(self, path):\n",
    "        self.main_path = path\n",
    "        self.left_camera_images = sorted(os.listdir(os.path.join(self.main_path, \"image_02/data\")))\n",
    "        self.right_camera_images = sorted(os.listdir(os.path.join(self.main_path, \"image_03/data\")))\n",
    "        print(self.left_camera_images)\n",
    "        self.frames = len(self.left_camera_images)\n",
    "\n",
    "        calibration = pd.read_csv(os.path.join(self.main_path, \"calib_cam_to_cam.txt\"), delimiter=' ', header=None, index_col=0)\n",
    "        #projection matrix\n",
    "        self.P0 = np.array(calibration.loc['P_rect_02:'], dtype=np.float32).reshape((3,4))\n",
    "        self.P1 = np.array(calibration.loc['P_rect_03:'], dtype=np.float32).reshape((3,4))\n",
    "\n",
    "        #image loader\n",
    "        self.left_images = []\n",
    "        self.right_images = []\n",
    "\n",
    "        for i, left in enumerate(self.left_camera_images):\n",
    "            right = self.right_camera_images[i]\n",
    "            self.left_images.append(cv2.imread((os.path.join(self.main_path, \"image_02/data/\")) + left))\n",
    "            self.right_images.append(cv2.imread((os.path.join(self.main_path, \"image_03/data/\")) + right))\n",
    "        \n",
    "        self.first_image_left = self.left_images[0]\n",
    "        self.first_image_right = self.right_images[0]\n",
    "        self.second_image_left = self.left_images[1]\n",
    "        self.second_image_right = self.right_images[1]\n",
    "\n",
    "        self.image_height = self.left_images[0].shape[0]\n",
    "        self.image_width = self.left_images[0].shape[1]\n",
    "        "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Sezione Utils: Funzioni di Supporto\n",
    "\n",
    "Questa sezione contiene una raccolta di funzioni utili e modulari che vengono utilizzate in varie parti del progetto per migliorare la leggibilità e la modularità del codice. \n",
    "Di seguito viene fornita una panoramica delle principali funzionalità implementate:\n",
    "\n",
    "### 1. **Caricamento della Configurazione**\n",
    "Inizialmente vengono caricati dei parametri da un file YAML **(initial_config.yaml)** per rendere il codice facilmente configurabile e adattabile. Questi parametri vengono utilizzati nelle funzioni per definire aspetti come la profondità massima, il nome del rilevatore di feature (SIFT o ORB) e altre impostazioni.\n",
    "\n",
    "### 2. **Mappatura della Disparità**\n",
    "La funzione **disparity_mapping** calcola la mappa di disparità tra due immagini stereo utilizzando l'algoritmo StereoSGBM di OpenCV.\n",
    "\n",
    "### 3. **Decomposizione della Matrice di Proiezione**\n",
    "La funzione **decomposition** estrae la matrice intrinseca, la matrice di rotazione e il vettore di traslazione da una matrice di proiezione, rendendo queste informazioni accessibili per calcoli futuri.\n",
    "\n",
    "### 4. **Mappatura della Profondità**\n",
    "La funzione **depth_mapping** calcola la profondità a partire dalla mappa di disparità. Utilizza la lunghezza focale e il baseline della telecamera per convertire i pixel della disparità in coordinate di profondità reali.\n",
    "\n",
    "### 5. **Estrazione e Matching delle Feature**\n",
    "Le funzioni **feature_extractor** e **feature_matching** consentono di rilevare punti caratteristici in un'immagine utilizzando algoritmi come SIFT o ORB e di associare tali punti tra immagini sequenziali. Questi passaggi sono cruciali per il tracking e il motion estimation.\n",
    "\n",
    "### 6. **Visualizzazione dei Match**\n",
    "La funzione **visualize_matches** permette di visualizzare i punti corrispondenti tra due immagini, semplificando il debugging e la comprensione del processo di matching.\n",
    "\n",
    "### 7. **Stima del Movimento Della Telecamera**\n",
    "La funzione **motion_estimation** calcola la stima del movimento della telecamera tra due fotogrammi sequenziali. Utilizza i punti 3D derivati dalla mappa di profondità e applica l'algoritmo RANSAC per rimuovere gli outlier, migliorando l'accuratezza della stima.\n",
    "\n",
    "### 8. **Pulizia dei Punti 3D**\n",
    "La funzione **pointCleaning** viene utilizzata per filtrare i punti 3D visualizzati nella mappa 3D degli oggetti rilevati durante il video. Utilizza un KD-Tree per identificare e rimuovere punti rumorosi, al fine di avere una visualizzazione dei dati più pulita e accurata.\n",
    "\n",
    "\n",
    "### 9. **Gestione dei File**\n",
    "La funzione **get_next_filename** genera in modo incrementale nomi di file unici all'interno di una cartella specificata.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(\"./initial_config.yaml\", \"r\") as stream:\n",
    "    try:\n",
    "        config = yaml.safe_load(stream)\n",
    "    except yaml.YAMLError as error:\n",
    "        print(error)\n",
    "\n",
    "rgb_value = config['parameters']['rgb']\n",
    "rectified_value = config['parameters']['rectified']\n",
    "detector_name = config['parameters']['detector']\n",
    "max_depth_value = config['parameters']['max_depth']\n",
    "\n",
    "##stereo depth estimation\n",
    "\n",
    "def disparity_mapping(left_image, right_image, rgb=rgb_value):\n",
    "    if rgb:\n",
    "        num_channels = 3\n",
    "    else:\n",
    "        num_channels = 1\n",
    "\n",
    "    num_disparities = 6*16\n",
    "    block_size = 7\n",
    "\n",
    "    matcher = cv2.StereoSGBM_create(numDisparities = num_disparities, \n",
    "                                    minDisparity = 0, \n",
    "                                    blockSize = block_size, \n",
    "                                    P1=8 * num_channels * block_size ** 2,\n",
    "                                    P2=32 * num_channels * block_size ** 2,\n",
    "                                    mode = cv2.STEREO_SGBM_MODE_SGBM_3WAY)\n",
    "    if rgb:\n",
    "        left_image = cv2.cvtColor(left_image, cv2.COLOR_BGR2GRAY)\n",
    "        right_image = cv2.cvtColor(right_image, cv2.COLOR_BGR2GRAY)\n",
    "    \n",
    "    left_image_disparity_map = matcher.compute(left_image, right_image).astype(np.float32)/16\n",
    "    #plt.title(\"disparity mapping\")\n",
    "    #plt.imshow(left_image_disparity_map)\n",
    "\n",
    "    return left_image_disparity_map\n",
    "\n",
    "def decomposition(p):\n",
    "\n",
    "    intrinsic_matrix, rotation_matrix, translation_vector, _, _, _, _ = cv2.decomposeProjectionMatrix(p)\n",
    "\n",
    "    translation_vector = (translation_vector / translation_vector[3])[:3]\n",
    "\n",
    "    return intrinsic_matrix, rotation_matrix, translation_vector\n",
    "\n",
    "def depth_mapping(left_disparity_map, left_intrinsic, left_translation, right_translation, rectified=rectified_value):\n",
    "\n",
    "    focal_length = left_intrinsic[0][0]\n",
    "\n",
    "    if rectified:\n",
    "        baseline = right_translation[0] - left_translation[0]\n",
    "    else:\n",
    "        baseline = left_translation[0] - right_translation[0]\n",
    "\n",
    "    left_disparity_map[left_disparity_map == 0.0] = 0.1\n",
    "    left_disparity_map[left_disparity_map == -1.0] = 0.1\n",
    "\n",
    "    depth_map = np.ones(left_disparity_map.shape)\n",
    "    depth_map = (focal_length * baseline) / left_disparity_map\n",
    "\n",
    "    return depth_map\n",
    "\n",
    "def stereo_depth(left_image, right_image, P0, P1, rgb=rgb_value):\n",
    "\n",
    "    disp_map = disparity_mapping(left_image, right_image, rgb=rgb)\n",
    "\n",
    "    l_intrinsic, l_rotation, l_translation = decomposition(P0)\n",
    "    r_intrinsic, r_rotation, r_translation = decomposition(P1)\n",
    "\n",
    "    depth = depth_mapping(disp_map, l_intrinsic, l_translation, r_translation)\n",
    "\n",
    "    return depth\n",
    "\n",
    "##stereo depth estimation\n",
    "\n",
    "\n",
    "##feature extraction and matching\n",
    "\n",
    "def feature_extractor(image, detector=detector_name,mask=None):\n",
    "\n",
    "    if detector == 'sift':\n",
    "        create_detector = cv2.SIFT_create()\n",
    "    elif detector == 'orb':\n",
    "        create_detector = cv2.ORB_create()\n",
    "\n",
    "    keypoints, descriptors = create_detector.detectAndCompute(image, mask)\n",
    "\n",
    "    return keypoints, descriptors\n",
    "\n",
    "def feature_matching(first_descriptor, second_descriptor, detector=detector_name, k=2, distance_threshold=1.0):\n",
    "\n",
    "    if detector == 'sift':\n",
    "        feature_matcher = cv2.BFMatcher_create(cv2.NORM_L2, crossCheck = False)\n",
    "    elif detector == 'orb':\n",
    "        feature_matcher = cv2.BFMatcher_create(cv2.NORM_L2, crossCheck = False)\n",
    "    matches = feature_matcher.knnMatch(first_descriptor, second_descriptor, k=k)\n",
    "    filtered_matches = []\n",
    "    for match1, match2 in matches:\n",
    "        if match1.distance <= distance_threshold * match2.distance:\n",
    "            filtered_matches.append(match1)\n",
    "\n",
    "    return filtered_matches\n",
    "\n",
    "def visualize_matches(first_image, second_image, keypoint_one, keypoint_two, matches):\n",
    "    show_matches = cv2.drawMatches(first_image, keypoint_one, second_image, keypoint_two, matches, None, flags = 2)\n",
    "    plt.figure(figsize=(15, 5), dpi=100)\n",
    "    plt.imshow(show_matches)\n",
    "    plt.show()\n",
    "\n",
    "##feature extraction and matching\n",
    "\n",
    "##motion estimation\n",
    "\n",
    "def motion_estimation(matches, firstImage_keypoints, secondImage_keypoints, intrinsic_matrix, depth, max_depth=max_depth_value):\n",
    "    \"\"\"\n",
    "    Estimating motion of the left camera from sequential imgaes \n",
    "\n",
    "    \"\"\"\n",
    "    rotation_matrix = np.eye(3)\n",
    "    translation_vector = np.zeros((3, 1))\n",
    "\n",
    "    # Only considering keypoints that are matched for two sequential frames\n",
    "    image1_points = np.float32(\n",
    "        [firstImage_keypoints[m.queryIdx].pt for m in matches])\n",
    "    image2_points = np.float32(\n",
    "        [secondImage_keypoints[m.trainIdx].pt for m in matches])\n",
    "\n",
    "    cx = intrinsic_matrix[0, 2]\n",
    "    cy = intrinsic_matrix[1, 2]\n",
    "    fx = intrinsic_matrix[0, 0]\n",
    "    fy = intrinsic_matrix[1, 1]\n",
    "\n",
    "    points_3D = np.zeros((0, 3))\n",
    "    outliers = []\n",
    "\n",
    "    # Extract depth information to build 3D positions\n",
    "    for indices, (u, v) in enumerate(image1_points):\n",
    "        z = depth[int(v), int(u)]\n",
    "\n",
    "        # We will not consider depth greater than max_depth\n",
    "        if z > max_depth:\n",
    "            outliers.append(indices)\n",
    "            continue\n",
    "\n",
    "        # Using z we can find the x,y points in 3D coordinate using the formula\n",
    "        x = z*(u-cx)/fx\n",
    "        y = z*(v-cy)/fy\n",
    "\n",
    "        # Stacking all the 3D (x,y,z) points\n",
    "        points_3D = np.vstack([points_3D, np.array([x, y, z])])\n",
    "\n",
    "    # Deleting the false depth points\n",
    "    image1_points = np.delete(image1_points, outliers, 0)\n",
    "    image2_points = np.delete(image2_points, outliers, 0)\n",
    "\n",
    "    # Apply Ransac Algorithm to remove outliers\n",
    "    _, rvec, translation_vector, _ = cv2.solvePnPRansac(\n",
    "        points_3D, image2_points, intrinsic_matrix, None)\n",
    "\n",
    "    rotation_matrix = cv2.Rodrigues(rvec)[0]\n",
    "\n",
    "    return rotation_matrix, translation_vector, image1_points, image2_points\n",
    "\n",
    "def pointCleaning(points, threshold):\n",
    "    var = 0\n",
    "    \n",
    "    kdtree = cKDTree(points)\n",
    "    cleaned_points = []\n",
    "\n",
    "    for i, point in enumerate(points):\n",
    "        neighbour = kdtree.query(point, k=100, distance_upper_bound=threshold)\n",
    "\n",
    "        distance = neighbour[0]\n",
    "        idxs = neighbour[1]\n",
    "\n",
    "        distance = distance[distance < threshold]\n",
    "\n",
    "        if len(distance) > 50:\n",
    "            cleaned_points.append(point)\n",
    "    if var == 1:\n",
    "        for point in cleaned_points:\n",
    "            point = np.array([point[0], point[1], point[2], 1])\n",
    "    return cleaned_points\n",
    "\n",
    "def get_next_filename(folder, base_name, ext):\n",
    "    index = 1\n",
    "    while True:\n",
    "        filename = base_name + str(index) + ext\n",
    "        filepath = os.path.join(folder, filename)\n",
    "        if not os.path.exists(filepath):\n",
    "            return filepath\n",
    "        index += 1\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Elaborazione con YOLOv8\n",
    "\n",
    "Questa sezione si occupa dell'elaborazione dei frame video utilizzando il modello di rilevamento YOLOv11. L'obiettivo principale è rilevare oggetti nella scena, restituendo informazioni dettagliate su ciascun oggetto rilevato.\n",
    "\n",
    "![Immagine non disponibile](references/yolo_results.png)\n",
    "\n",
    "L'opzione **persist=True**, all'interno di yolo_model.track, consente di mantenere le informazioni sul tracciamento tra i frame.\n",
    "\n",
    "Infine viene restituita una lista contenente le informazioni sugli oggetti rilevati e un'altra lista contenente gli id di questi.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "yolo_model = YOLO('yolo11n.pt') "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def yolo_elaboration(frame):\n",
    "    detection_info = []\n",
    "    detection_id = []\n",
    "\n",
    "    yolo_results = yolo_model.track(frame, persist=True)\n",
    "\n",
    "    if not yolo_results[0]:\n",
    "        return detection_info, detection_id \n",
    "    \n",
    "\n",
    "    #risultati del rilevamento\n",
    "    detection = yolo_results[0].boxes.xyxy #coordinate della bounding box\n",
    "    confidences = yolo_results[0].boxes.conf #confidenza della rilevazione\n",
    "    class_ids = yolo_results[0].boxes.cls #ID delle classi di appartenenza dell'oggetto\n",
    "    obj_ids = yolo_results[0].boxes.id #ID dell'oggetto\n",
    "    \n",
    "    #ciclo di gestione dei risultati \n",
    "    \n",
    "    for i, (box, conf, cls_id, obj_id) in enumerate(zip(detection, confidences, class_ids, obj_ids)):\n",
    "        x1, y1, x2, y2 = map(int, box) #estrazione delle le coordinate della bounding box\n",
    "\n",
    "        label = f\"{yolo_model.names[int(cls_id)]} ({conf:.2f})\" \n",
    "\n",
    "        detection_info.append({\n",
    "            'id': obj_id,\n",
    "            'label': label,\n",
    "            'confidence': conf,\n",
    "            'class': cls_id,\n",
    "            'x1': x1,\n",
    "            'x2': x2,\n",
    "            'y1': y1,\n",
    "            'y2': y2\n",
    "        })\n",
    "        detection_id.append(obj_id)\n",
    "    #annotated_frame = yolo_results[0].plot()\n",
    "    #plt.title(\"yolo\")\n",
    "    #plt.imshow(annotated_frame)\n",
    "    return detection_info, detection_id"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Elaborazione di Oggetti e Coordinate 3D\n",
    "\n",
    "Questa sezione contiene due funzioni principali per il calcolo  delle coordinate 3D degli oggetti rilevati precedentemente.\n",
    "\n",
    "---\n",
    "\n",
    "In particolare la funzione `_elaboration` calcola le coordinate 3D degli oggetti rilevati all'interno di una bounding box e le trasforma nel sistema di riferimento globale.\n",
    "\n",
    "### Funzionamento\n",
    "\n",
    "Vengono estratti i parametri della telecamera (`cx`, `cy`, `fx`, `fy`) dalla matrice intrinseca.\n",
    "   \n",
    "Per ogni pixel campionato (20% dei pixel totali per alleggerire i calcoli):\n",
    "   - La profondità z viene recuperata dalla mappa di profondità (depth_map).\n",
    "   - Le coordinate 3D reali x,y vengono calcolate usando le seguenti formule:\n",
    "\n",
    "        #### $x_{real} = \\frac{(x - c_x) \\cdot depth}{f_x}$;\n",
    "        #### $y_{real} = \\frac{(y - c_y) \\cdot depth}{f_y}$.\n",
    "\n",
    "Le coordinate 3D vengono trasformate nel sistema di riferimento globale utilizzando la homogenous matrix e aggiunte alla lista object_data.\n",
    "\n",
    "### Output\n",
    "- Restituisce:\n",
    "  - object_data: coordinate 3D dell'oggetto.\n",
    "  - label: etichetta dell'oggetto.\n",
    "  - confidence: livello di confidenza del rilevamento.\n",
    "\n",
    "---\n",
    "\n",
    "La funzione `elaboration` aggiorna i dati degli oggetti rilevati combinando i rilevamenti attuali con quelli passati.\n",
    "\n",
    "### Funzionamento\n",
    "\n",
    "Inizialmente viene fatto un controllo su current_detection, se non ci sono nuovi rilevamenti, la funzione restituisce old_detection.\n",
    "\n",
    "Poi per ogni oggetto nei rilevamenti attuali:\n",
    "   - Se l'oggetto è già stato rilevato (obj_id in old_detection_id), le coordinate 3D vengono aggiornate combinando i dati attuali e passati.\n",
    "   - Se l'oggetto non è stato rilevato in precedenza, un nuovo oggetto viene creato con le sue coordinate 3D calcolate e viene aggiunto a old_detection.\n",
    "\n",
    "### Output\n",
    "- Restituisce old_detection aggiornato, contenente:\n",
    "  - ID dell'oggetto\n",
    "  - Classe dell'oggetto.\n",
    "  - label\n",
    "  - Livelli di confidenza.\n",
    "  - Coordinate 3D.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "def _elaboration(object, object_data, depth_map, transformation_matrix, intrinsic_matrix):\n",
    "    cx = intrinsic_matrix[0, 2]\n",
    "    cy = intrinsic_matrix[1, 2]\n",
    "    fx = intrinsic_matrix[0, 0]\n",
    "    fy = intrinsic_matrix[1, 1]\n",
    "    label = object['label']\n",
    "    confidence = object['confidence']\n",
    "    x1 = object['x1']\n",
    "    x2 = object['x2']\n",
    "    y1 = object['y1']\n",
    "    y2 = object['y2']\n",
    "    #if confidence < 0.20:\n",
    "        #continue\n",
    "    for y in range(y1, y2):\n",
    "        for x in range(x1, x2):\n",
    "            #campiono il 20% dei pixel all'interno della bounding box\n",
    "            if random.random() > 0.20:\n",
    "                continue\n",
    "            depth = depth_map[y,x]\n",
    "            if depth > 100:\n",
    "                continue\n",
    "            real_x = ((x - cx) * depth)/fx\n",
    "            real_y = ((y - cy) * depth)/fy\n",
    "            obj_coords = np.array([real_x, real_y, depth, 1])\n",
    "            world_obj_coords = transformation_matrix.dot(obj_coords)\n",
    "            object_data.append(world_obj_coords)\n",
    "    return object_data, label, confidence"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "def elaboration(depth_map, current_detection, old_detection, old_detection_id, intrinsic_matrix, transformation_matrix):\n",
    "    if not current_detection:\n",
    "        return old_detection\n",
    "    detection_info = current_detection\n",
    "    \n",
    "    for object in detection_info:\n",
    "        obj_id = object['id'] \n",
    "        obj_class = object['class']\n",
    "        #se l'oggetto rilevato e' stato gia scansionato precedentemente, ne miglioro la posizione aggiungendo le coordinate dei pixel della scansione nuova\n",
    "        if obj_id in old_detection_id:\n",
    "            \n",
    "            object_data = []\n",
    "            for obj in old_detection:\n",
    "                if obj['id'] == obj_id:\n",
    "                    object_data = obj['3dcoord']\n",
    "                    break\n",
    "            object_data, label, confidence = _elaboration(object, object_data, depth_map, transformation_matrix, intrinsic_matrix)\n",
    "            \n",
    "            for i, obj in enumerate(old_detection):\n",
    "                if obj['id'] == obj_id:\n",
    "                    old_detection[i] = {'id': obj_id,\n",
    "                                        'class': obj_class,\n",
    "                                        'label': label,\n",
    "                                        'confidence': confidence,\n",
    "                                        '3dcoord': object_data}\n",
    "            continue\n",
    "        \n",
    "        #se l'oggetto scansionato non era stato scansionato precedentemente, aggiungo un nuovo elemento nella lista old_detection che contiene varie informazioni tra cui l'id e le coordinate dei pixel del nuovo oggetto\n",
    "        object_data = []\n",
    "        object_data, label, confidence = _elaboration(object, object_data, depth_map, transformation_matrix, intrinsic_matrix)\n",
    "        old_detection.append({\n",
    "            'id' : obj_id,\n",
    "            'class': obj_class,\n",
    "            'label' : label,\n",
    "            'confidence' : confidence,\n",
    "            '3dcoord' : object_data\n",
    "        })\n",
    "    return old_detection\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## threedMapConversion\n",
    "\n",
    "La funzione `threedMapConversion` ha il compito di inserire le coordinate 3D calcolate in una mappa tridimensionale visualizzabile. Utilizza il modulo Open3D per creare una point cloud rappresentante la scena rilevata.\n",
    "\n",
    "### Funzionamento\n",
    "\n",
    "   Per ciascun oggetto in detection_info, vengono estratte le coordinate tridimensionali dai dati disponibili, escludendo il quarto elemento (che rappresenta un punto omogeneo).\n",
    "   Se la lista è vuota, significa che non sono stati scansionati oggetti nella scena. In tal caso, viene stampato un messaggio di avviso e la funzione termina.\n",
    "   Successivamente la funzione pointCleaning fa una pulizia dei punti al fine di rendere la visualizzazione piu' comprensibile\n",
    "   Le coordinate della telecamera vengono aggiunte ai punti già raccolti per includerle nella rappresentazione finale al fine di vedere anche lo spostamento  che la macchina ha compiuto.\n",
    "   che rappresenta la nuvola di punti tridimensionale.\n",
    "   I punti nella lista poi vengono usati per creare un oggetto PointCloud che rappresenta una nuvola di punti, che viene salvata come file .ply   \n",
    "\n",
    "![Immagine non disponibile](references/pointcloud2.png)\n",
    "\n",
    "---\n",
    "\n",
    "![Immagine non disponibile](references/pointcloudtop.png)\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "def threedMapConversion(detection_info, camera_coordinates,):\n",
    "    points = []\n",
    "    for object in detection_info:\n",
    "        threedcoords = [coord[:3] for coord in object['3dcoord']]\n",
    "        points.extend(threedcoords)\n",
    "    if not points:\n",
    "        print(\"Non sono stati scansionati oggetti\")\n",
    "        return\n",
    "    points = pointCleaning(points, 0.2)\n",
    "    points.extend(camera_coordinates)\n",
    "    points = np.array(points)\n",
    "    pcd = o3d.geometry.PointCloud()\n",
    "    pcd.points = o3d.utility.Vector3dVector(points)\n",
    "    o3d.visualization.draw_geometries([pcd])\n",
    "\n",
    "    output_path = get_next_filename(\"renders\", \"output\", \".ply\")\n",
    "    print(output_path)\n",
    "    o3d.io.write_point_cloud(output_path, pcd)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "pcd = o3d.io.read_point_cloud(\"renders/output2.ply\")\n",
    "o3d.visualization.draw_geometries([pcd])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "La seguente funzione contiene un ciclo per ogni frame del video in cui vengono eseguiti diversi compiti:\n",
    "\n",
    "### Calcolo della traiettoria della telecamera\n",
    "\n",
    "Viene calcolata la depth map a partire dalla disparity map come spiegato in precedenza, successivamente viene stimato il movimento della telecamera per frame sequenziali e aggiornata la homogenous matrix (matrice contenente la rotazione e la traslazione della telecamera).\n",
    "\n",
    "### Rilevamento di oggetti nella scena\n",
    "\n",
    "Tramite YOLO (You only look once) viene fatta una scansione degli oggetti presenti nel frame, i dati raccolti vengono passati nella funzione elaboration che conserva le coordinate 3d dei punti nella bounding box dell'oggetto\n",
    "\n",
    "---\n",
    "\n",
    "Alla fine del ciclo, le informazioni su tutti gli oggetti scansionati sono usate per generare una mappa 3d che verra' salvata nella cartella renders in formato .ply\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(\"./initial_config.yaml\", \"r\") as stream:\n",
    "    try:\n",
    "        config = yaml.safe_load(stream)\n",
    "    except yaml.YAMLError as error:\n",
    "        print(error)\n",
    "\n",
    "detector_name = config['parameters']['detector']\n",
    "subset = config['parameters']['subset']\n",
    "threshold = config['parameters']['distance_threshold']\n",
    "\n",
    "def odometry(data_handler, detector = detector_name, mask = None, subset = subset, plot = True):\n",
    "\n",
    "    if subset is not None:\n",
    "        num_frames = subset\n",
    "    else:\n",
    "        num_frames = data_handler.frames\n",
    "\n",
    "    # Create a homogeneous matrix\n",
    "    homo_matrix = np.eye(4)\n",
    "    trajectory = np.zeros((num_frames, 3, 4))\n",
    "    trajectory[0] = homo_matrix[:3, :]\n",
    "    camera_coordinates = []\n",
    "\n",
    "    # From projection matrix retrieve the left camera's intrinsic matrix\n",
    "    left_intrinsic_matrix, _, _ = decomposition(data_handler.P0)\n",
    "\n",
    "    #yolo\n",
    "    detection = []\n",
    "    detection_id = []\n",
    "\n",
    "    # Loop to iterate all the frames\n",
    "    for i in range(num_frames - 1):\n",
    "\n",
    "        image_left = data_handler.left_images[i]\n",
    "        image_right = data_handler.right_images[i]\n",
    "        next_image = data_handler.left_images[i + 1]\n",
    "\n",
    "        # Estimating the depth of an image\n",
    "        depth = stereo_depth(image_left, image_right, P0=data_handler.P0, P1=data_handler.P1)\n",
    "        #plt.title(\"Stereo Depth Mapping\")\n",
    "        #plt.figure(figsize=(12,6))\n",
    "        #plt.imshow(depth)\n",
    "        #print(depth)\n",
    "        \n",
    "        # Keypoints and Descriptors of two sequential images of the left camera\n",
    "        keypoint_left_first, descriptor_left_first = feature_extractor(image_left, detector, mask)\n",
    "        keypoint_left_next, descriptor_left_next = feature_extractor(next_image, detector, mask)\n",
    "\n",
    "        # Use feature (e.g. SIFT or ORB) detector to match features\n",
    "        matches = feature_matching(descriptor_left_first, descriptor_left_next, detector=detector, distance_threshold=threshold)\n",
    "\n",
    "        # Estimate motion between sequential images of the left camera\n",
    "        rotation_matrix, translation_vector, _, _ = motion_estimation(matches, keypoint_left_first, keypoint_left_next, left_intrinsic_matrix, depth)\n",
    "\n",
    "        # Initialise a homogeneous matrix (4X4)\n",
    "        Transformation_matrix = np.eye(4)\n",
    "\n",
    "        # Build the Transformation matrix using rotation matrix and translation vector from motion estimation function\n",
    "        Transformation_matrix[:3, :3] = rotation_matrix\n",
    "        Transformation_matrix[:3, 3] = translation_vector.T\n",
    "\n",
    "        # Transformation wrt. world coordinate system\n",
    "        homo_matrix = homo_matrix.dot(np.linalg.inv(Transformation_matrix))\n",
    "\n",
    "        # Append the pose of camera in the trajectory array\n",
    "        trajectory[i+1, :, :] = homo_matrix[:3, :]\n",
    "        camera_coordinates.append((trajectory[i,0,3], trajectory[i,1,3], trajectory[i,2,3]))\n",
    "\n",
    "        #yolo\n",
    "        new_detection, new_detection_id = yolo_elaboration(image_left)\n",
    "\n",
    "        #elaboration\n",
    "        detection = elaboration(depth, new_detection, detection, detection_id, left_intrinsic_matrix, homo_matrix)\n",
    "\n",
    "        if new_detection_id is not None:\n",
    "            detection_id.extend(new_detection_id)\n",
    "        if i % 10 == 0:\n",
    "            print(f'{i} frames sono stati elaborati')\n",
    "\n",
    "        if i == num_frames - 2:\n",
    "            print('Tutti i frames sono stati elaborati')\n",
    "            break\n",
    "    threedMapConversion(detection, camera_coordinates)\n",
    "    return trajectory\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['0000000000.png', '0000000001.png', '0000000002.png', '0000000003.png', '0000000004.png', '0000000005.png', '0000000006.png', '0000000007.png', '0000000008.png', '0000000009.png', '0000000010.png', '0000000011.png', '0000000012.png', '0000000013.png', '0000000014.png', '0000000015.png', '0000000016.png', '0000000017.png', '0000000018.png', '0000000019.png', '0000000020.png', '0000000021.png', '0000000022.png', '0000000023.png', '0000000024.png', '0000000025.png', '0000000026.png', '0000000027.png', '0000000028.png', '0000000029.png', '0000000030.png', '0000000031.png', '0000000032.png', '0000000033.png', '0000000034.png', '0000000035.png', '0000000036.png', '0000000037.png', '0000000038.png', '0000000039.png', '0000000040.png', '0000000041.png', '0000000042.png', '0000000043.png', '0000000044.png', '0000000045.png', '0000000046.png', '0000000047.png', '0000000048.png', '0000000049.png', '0000000050.png', '0000000051.png', '0000000052.png', '0000000053.png', '0000000054.png', '0000000055.png', '0000000056.png', '0000000057.png', '0000000058.png', '0000000059.png', '0000000060.png', '0000000061.png', '0000000062.png', '0000000063.png', '0000000064.png', '0000000065.png', '0000000066.png', '0000000067.png', '0000000068.png', '0000000069.png', '0000000070.png', '0000000071.png', '0000000072.png', '0000000073.png', '0000000074.png', '0000000075.png', '0000000076.png', '0000000077.png', '0000000078.png', '0000000079.png', '0000000080.png', '0000000081.png', '0000000082.png', '0000000083.png', '0000000084.png', '0000000085.png', '0000000086.png', '0000000087.png', '0000000088.png', '0000000089.png', '0000000090.png', '0000000091.png', '0000000092.png', '0000000093.png', '0000000094.png', '0000000095.png', '0000000096.png', '0000000097.png', '0000000098.png', '0000000099.png', '0000000100.png', '0000000101.png', '0000000102.png', '0000000103.png', '0000000104.png', '0000000105.png', '0000000106.png', '0000000107.png', '0000000108.png', '0000000109.png', '0000000110.png', '0000000111.png', '0000000112.png', '0000000113.png', '0000000114.png', '0000000115.png', '0000000116.png', '0000000117.png', '0000000118.png', '0000000119.png', '0000000120.png', '0000000121.png', '0000000122.png', '0000000123.png', '0000000124.png', '0000000125.png', '0000000126.png', '0000000127.png', '0000000128.png', '0000000129.png', '0000000130.png']\n",
      "\n",
      "0: 224x640 5 cars, 23.5ms\n",
      "Speed: 2.5ms preprocess, 23.5ms inference, 8.0ms postprocess per image at shape (1, 3, 224, 640)\n",
      "0 frames sono stati elaborati\n",
      "\n",
      "0: 224x640 1 person, 5 cars, 29.5ms\n",
      "Speed: 1.5ms preprocess, 29.5ms inference, 10.0ms postprocess per image at shape (1, 3, 224, 640)\n",
      "\n",
      "0: 224x640 1 person, 6 cars, 9.0ms\n",
      "Speed: 1.0ms preprocess, 9.0ms inference, 1.5ms postprocess per image at shape (1, 3, 224, 640)\n",
      "\n",
      "0: 224x640 6 cars, 9.0ms\n",
      "Speed: 1.0ms preprocess, 9.0ms inference, 1.5ms postprocess per image at shape (1, 3, 224, 640)\n",
      "\n",
      "0: 224x640 6 cars, 9.0ms\n",
      "Speed: 1.0ms preprocess, 9.0ms inference, 1.5ms postprocess per image at shape (1, 3, 224, 640)\n",
      "\n",
      "0: 224x640 6 cars, 8.0ms\n",
      "Speed: 1.0ms preprocess, 8.0ms inference, 1.0ms postprocess per image at shape (1, 3, 224, 640)\n",
      "\n",
      "0: 224x640 7 cars, 8.0ms\n",
      "Speed: 1.0ms preprocess, 8.0ms inference, 1.0ms postprocess per image at shape (1, 3, 224, 640)\n",
      "\n",
      "0: 224x640 6 cars, 1 truck, 7.5ms\n",
      "Speed: 0.5ms preprocess, 7.5ms inference, 1.0ms postprocess per image at shape (1, 3, 224, 640)\n",
      "\n",
      "0: 224x640 6 cars, 1 truck, 7.0ms\n",
      "Speed: 1.0ms preprocess, 7.0ms inference, 1.5ms postprocess per image at shape (1, 3, 224, 640)\n",
      "\n",
      "0: 224x640 6 cars, 1 truck, 9.0ms\n",
      "Speed: 1.5ms preprocess, 9.0ms inference, 2.5ms postprocess per image at shape (1, 3, 224, 640)\n",
      "\n",
      "0: 224x640 6 cars, 1 truck, 9.0ms\n",
      "Speed: 1.5ms preprocess, 9.0ms inference, 2.5ms postprocess per image at shape (1, 3, 224, 640)\n",
      "10 frames sono stati elaborati\n",
      "\n",
      "0: 224x640 5 cars, 1 truck, 14.5ms\n",
      "Speed: 1.5ms preprocess, 14.5ms inference, 4.0ms postprocess per image at shape (1, 3, 224, 640)\n",
      "\n",
      "0: 224x640 5 cars, 1 truck, 16.5ms\n",
      "Speed: 2.0ms preprocess, 16.5ms inference, 5.0ms postprocess per image at shape (1, 3, 224, 640)\n",
      "\n",
      "0: 224x640 5 cars, 1 truck, 15.0ms\n",
      "Speed: 2.0ms preprocess, 15.0ms inference, 3.0ms postprocess per image at shape (1, 3, 224, 640)\n",
      "\n",
      "0: 224x640 5 cars, 1 truck, 14.5ms\n",
      "Speed: 1.5ms preprocess, 14.5ms inference, 3.5ms postprocess per image at shape (1, 3, 224, 640)\n",
      "\n",
      "0: 224x640 6 cars, 1 truck, 17.5ms\n",
      "Speed: 1.5ms preprocess, 17.5ms inference, 3.0ms postprocess per image at shape (1, 3, 224, 640)\n",
      "\n",
      "0: 224x640 6 cars, 1 truck, 14.0ms\n",
      "Speed: 1.5ms preprocess, 14.0ms inference, 3.0ms postprocess per image at shape (1, 3, 224, 640)\n",
      "\n",
      "0: 224x640 7 cars, 1 truck, 12.0ms\n",
      "Speed: 1.5ms preprocess, 12.0ms inference, 2.5ms postprocess per image at shape (1, 3, 224, 640)\n",
      "\n",
      "0: 224x640 7 cars, 2 trucks, 13.5ms\n",
      "Speed: 1.5ms preprocess, 13.5ms inference, 3.0ms postprocess per image at shape (1, 3, 224, 640)\n",
      "\n",
      "0: 224x640 6 cars, 2 trucks, 13.0ms\n",
      "Speed: 1.5ms preprocess, 13.0ms inference, 3.0ms postprocess per image at shape (1, 3, 224, 640)\n",
      "\n",
      "0: 224x640 6 cars, 2 trucks, 12.0ms\n",
      "Speed: 1.5ms preprocess, 12.0ms inference, 3.0ms postprocess per image at shape (1, 3, 224, 640)\n",
      "20 frames sono stati elaborati\n",
      "\n",
      "0: 224x640 5 cars, 1 truck, 15.5ms\n",
      "Speed: 1.5ms preprocess, 15.5ms inference, 4.5ms postprocess per image at shape (1, 3, 224, 640)\n",
      "\n",
      "0: 224x640 6 cars, 1 truck, 12.5ms\n",
      "Speed: 1.5ms preprocess, 12.5ms inference, 3.5ms postprocess per image at shape (1, 3, 224, 640)\n",
      "\n",
      "0: 224x640 6 cars, 2 trucks, 12.5ms\n",
      "Speed: 1.5ms preprocess, 12.5ms inference, 3.5ms postprocess per image at shape (1, 3, 224, 640)\n",
      "\n",
      "0: 224x640 6 cars, 2 trucks, 12.5ms\n",
      "Speed: 1.5ms preprocess, 12.5ms inference, 3.5ms postprocess per image at shape (1, 3, 224, 640)\n",
      "\n",
      "0: 224x640 5 cars, 2 trucks, 13.0ms\n",
      "Speed: 1.5ms preprocess, 13.0ms inference, 3.0ms postprocess per image at shape (1, 3, 224, 640)\n",
      "\n",
      "0: 224x640 5 cars, 2 trucks, 13.0ms\n",
      "Speed: 2.0ms preprocess, 13.0ms inference, 3.0ms postprocess per image at shape (1, 3, 224, 640)\n",
      "\n",
      "0: 224x640 5 cars, 1 bus, 1 truck, 14.0ms\n",
      "Speed: 1.5ms preprocess, 14.0ms inference, 4.0ms postprocess per image at shape (1, 3, 224, 640)\n",
      "\n",
      "0: 224x640 5 cars, 2 trucks, 13.5ms\n",
      "Speed: 1.0ms preprocess, 13.5ms inference, 2.5ms postprocess per image at shape (1, 3, 224, 640)\n",
      "\n",
      "0: 224x640 6 cars, 2 trucks, 13.0ms\n",
      "Speed: 1.5ms preprocess, 13.0ms inference, 3.5ms postprocess per image at shape (1, 3, 224, 640)\n",
      "\n",
      "0: 224x640 6 cars, 2 trucks, 14.0ms\n",
      "Speed: 2.0ms preprocess, 14.0ms inference, 3.0ms postprocess per image at shape (1, 3, 224, 640)\n",
      "30 frames sono stati elaborati\n",
      "\n",
      "0: 224x640 5 cars, 3 trucks, 13.5ms\n",
      "Speed: 2.0ms preprocess, 13.5ms inference, 3.5ms postprocess per image at shape (1, 3, 224, 640)\n",
      "\n",
      "0: 224x640 6 cars, 2 trucks, 14.0ms\n",
      "Speed: 1.5ms preprocess, 14.0ms inference, 3.0ms postprocess per image at shape (1, 3, 224, 640)\n",
      "\n",
      "0: 224x640 6 cars, 2 trucks, 13.0ms\n",
      "Speed: 1.5ms preprocess, 13.0ms inference, 3.5ms postprocess per image at shape (1, 3, 224, 640)\n",
      "\n",
      "0: 224x640 5 cars, 3 trucks, 14.0ms\n",
      "Speed: 1.5ms preprocess, 14.0ms inference, 3.5ms postprocess per image at shape (1, 3, 224, 640)\n",
      "\n",
      "0: 224x640 7 cars, 3 trucks, 17.5ms\n",
      "Speed: 1.5ms preprocess, 17.5ms inference, 3.0ms postprocess per image at shape (1, 3, 224, 640)\n",
      "\n",
      "0: 224x640 9 cars, 1 truck, 12.0ms\n",
      "Speed: 2.0ms preprocess, 12.0ms inference, 4.0ms postprocess per image at shape (1, 3, 224, 640)\n",
      "\n",
      "0: 224x640 9 cars, 1 truck, 19.0ms\n",
      "Speed: 1.5ms preprocess, 19.0ms inference, 3.5ms postprocess per image at shape (1, 3, 224, 640)\n",
      "\n",
      "0: 224x640 9 cars, 15.5ms\n",
      "Speed: 1.5ms preprocess, 15.5ms inference, 4.5ms postprocess per image at shape (1, 3, 224, 640)\n",
      "\n",
      "0: 224x640 8 cars, 12.5ms\n",
      "Speed: 2.0ms preprocess, 12.5ms inference, 3.5ms postprocess per image at shape (1, 3, 224, 640)\n",
      "\n",
      "0: 224x640 7 cars, 1 truck, 9.5ms\n",
      "Speed: 0.5ms preprocess, 9.5ms inference, 1.0ms postprocess per image at shape (1, 3, 224, 640)\n",
      "40 frames sono stati elaborati\n",
      "\n",
      "0: 224x640 6 cars, 2 trucks, 7.0ms\n",
      "Speed: 1.0ms preprocess, 7.0ms inference, 1.0ms postprocess per image at shape (1, 3, 224, 640)\n",
      "\n",
      "0: 224x640 8 cars, 1 truck, 7.0ms\n",
      "Speed: 1.0ms preprocess, 7.0ms inference, 1.0ms postprocess per image at shape (1, 3, 224, 640)\n",
      "\n",
      "0: 224x640 10 cars, 7.5ms\n",
      "Speed: 1.0ms preprocess, 7.5ms inference, 1.5ms postprocess per image at shape (1, 3, 224, 640)\n",
      "\n",
      "0: 224x640 8 cars, 1 truck, 7.5ms\n",
      "Speed: 1.0ms preprocess, 7.5ms inference, 2.0ms postprocess per image at shape (1, 3, 224, 640)\n",
      "\n",
      "0: 224x640 10 cars, 9.0ms\n",
      "Speed: 1.5ms preprocess, 9.0ms inference, 2.5ms postprocess per image at shape (1, 3, 224, 640)\n",
      "\n",
      "0: 224x640 10 cars, 1 truck, 13.5ms\n",
      "Speed: 1.5ms preprocess, 13.5ms inference, 4.0ms postprocess per image at shape (1, 3, 224, 640)\n",
      "\n",
      "0: 224x640 10 cars, 1 truck, 15.0ms\n",
      "Speed: 1.5ms preprocess, 15.0ms inference, 4.0ms postprocess per image at shape (1, 3, 224, 640)\n",
      "\n",
      "0: 224x640 8 cars, 1 truck, 12.0ms\n",
      "Speed: 4.5ms preprocess, 12.0ms inference, 3.0ms postprocess per image at shape (1, 3, 224, 640)\n",
      "\n",
      "0: 224x640 7 cars, 1 truck, 13.5ms\n",
      "Speed: 1.5ms preprocess, 13.5ms inference, 1.5ms postprocess per image at shape (1, 3, 224, 640)\n",
      "\n",
      "0: 224x640 7 cars, 1 truck, 18.5ms\n",
      "Speed: 1.5ms preprocess, 18.5ms inference, 4.0ms postprocess per image at shape (1, 3, 224, 640)\n",
      "50 frames sono stati elaborati\n",
      "\n",
      "0: 224x640 8 cars, 1 truck, 7.0ms\n",
      "Speed: 1.0ms preprocess, 7.0ms inference, 1.5ms postprocess per image at shape (1, 3, 224, 640)\n",
      "\n",
      "0: 224x640 8 cars, 1 truck, 8.0ms\n",
      "Speed: 1.0ms preprocess, 8.0ms inference, 1.0ms postprocess per image at shape (1, 3, 224, 640)\n",
      "\n",
      "0: 224x640 9 cars, 2 trucks, 9.5ms\n",
      "Speed: 1.0ms preprocess, 9.5ms inference, 1.5ms postprocess per image at shape (1, 3, 224, 640)\n",
      "\n",
      "0: 224x640 10 cars, 1 truck, 9.0ms\n",
      "Speed: 1.0ms preprocess, 9.0ms inference, 1.5ms postprocess per image at shape (1, 3, 224, 640)\n",
      "\n",
      "0: 224x640 10 cars, 2 trucks, 9.0ms\n",
      "Speed: 1.0ms preprocess, 9.0ms inference, 1.5ms postprocess per image at shape (1, 3, 224, 640)\n",
      "\n",
      "0: 224x640 9 cars, 2 trucks, 9.0ms\n",
      "Speed: 0.5ms preprocess, 9.0ms inference, 1.5ms postprocess per image at shape (1, 3, 224, 640)\n",
      "\n",
      "0: 224x640 10 cars, 3 trucks, 11.0ms\n",
      "Speed: 1.0ms preprocess, 11.0ms inference, 1.5ms postprocess per image at shape (1, 3, 224, 640)\n",
      "\n",
      "0: 224x640 9 cars, 2 trucks, 10.0ms\n",
      "Speed: 1.0ms preprocess, 10.0ms inference, 1.5ms postprocess per image at shape (1, 3, 224, 640)\n",
      "\n",
      "0: 224x640 8 cars, 2 trucks, 8.5ms\n",
      "Speed: 1.0ms preprocess, 8.5ms inference, 2.5ms postprocess per image at shape (1, 3, 224, 640)\n",
      "\n",
      "0: 224x640 8 cars, 2 trucks, 9.5ms\n",
      "Speed: 1.5ms preprocess, 9.5ms inference, 2.0ms postprocess per image at shape (1, 3, 224, 640)\n",
      "60 frames sono stati elaborati\n",
      "\n",
      "0: 224x640 9 cars, 2 trucks, 13.0ms\n",
      "Speed: 1.5ms preprocess, 13.0ms inference, 3.0ms postprocess per image at shape (1, 3, 224, 640)\n",
      "\n",
      "0: 224x640 8 cars, 3 trucks, 12.5ms\n",
      "Speed: 1.5ms preprocess, 12.5ms inference, 3.5ms postprocess per image at shape (1, 3, 224, 640)\n",
      "\n",
      "0: 224x640 9 cars, 2 trucks, 13.0ms\n",
      "Speed: 1.5ms preprocess, 13.0ms inference, 3.0ms postprocess per image at shape (1, 3, 224, 640)\n",
      "\n",
      "0: 224x640 9 cars, 2 trucks, 13.0ms\n",
      "Speed: 1.5ms preprocess, 13.0ms inference, 3.0ms postprocess per image at shape (1, 3, 224, 640)\n",
      "\n",
      "0: 224x640 8 cars, 2 trucks, 12.0ms\n",
      "Speed: 1.5ms preprocess, 12.0ms inference, 3.0ms postprocess per image at shape (1, 3, 224, 640)\n",
      "\n",
      "0: 224x640 9 cars, 2 trucks, 13.0ms\n",
      "Speed: 1.5ms preprocess, 13.0ms inference, 3.0ms postprocess per image at shape (1, 3, 224, 640)\n",
      "\n",
      "0: 224x640 8 cars, 3 trucks, 12.5ms\n",
      "Speed: 1.5ms preprocess, 12.5ms inference, 3.5ms postprocess per image at shape (1, 3, 224, 640)\n",
      "\n",
      "0: 224x640 8 cars, 4 trucks, 25.0ms\n",
      "Speed: 1.5ms preprocess, 25.0ms inference, 6.0ms postprocess per image at shape (1, 3, 224, 640)\n",
      "\n",
      "0: 224x640 8 cars, 4 trucks, 30.0ms\n",
      "Speed: 1.5ms preprocess, 30.0ms inference, 6.5ms postprocess per image at shape (1, 3, 224, 640)\n",
      "\n",
      "0: 224x640 1 person, 6 cars, 4 trucks, 30.5ms\n",
      "Speed: 1.5ms preprocess, 30.5ms inference, 6.0ms postprocess per image at shape (1, 3, 224, 640)\n",
      "70 frames sono stati elaborati\n",
      "\n",
      "0: 224x640 7 cars, 4 trucks, 14.0ms\n",
      "Speed: 1.5ms preprocess, 14.0ms inference, 2.5ms postprocess per image at shape (1, 3, 224, 640)\n",
      "\n",
      "0: 224x640 6 cars, 4 trucks, 12.0ms\n",
      "Speed: 1.5ms preprocess, 12.0ms inference, 4.0ms postprocess per image at shape (1, 3, 224, 640)\n",
      "\n",
      "0: 224x640 6 cars, 3 trucks, 13.5ms\n",
      "Speed: 1.5ms preprocess, 13.5ms inference, 4.0ms postprocess per image at shape (1, 3, 224, 640)\n",
      "\n",
      "0: 224x640 8 cars, 2 trucks, 14.5ms\n",
      "Speed: 1.5ms preprocess, 14.5ms inference, 4.5ms postprocess per image at shape (1, 3, 224, 640)\n",
      "\n",
      "0: 224x640 8 cars, 3 trucks, 14.0ms\n",
      "Speed: 1.5ms preprocess, 14.0ms inference, 3.5ms postprocess per image at shape (1, 3, 224, 640)\n",
      "\n",
      "0: 224x640 9 cars, 4 trucks, 14.5ms\n",
      "Speed: 2.5ms preprocess, 14.5ms inference, 5.0ms postprocess per image at shape (1, 3, 224, 640)\n",
      "\n",
      "0: 224x640 10 cars, 2 trucks, 12.0ms\n",
      "Speed: 2.0ms preprocess, 12.0ms inference, 3.5ms postprocess per image at shape (1, 3, 224, 640)\n",
      "\n",
      "0: 224x640 7 cars, 1 truck, 13.0ms\n",
      "Speed: 1.5ms preprocess, 13.0ms inference, 3.5ms postprocess per image at shape (1, 3, 224, 640)\n",
      "\n",
      "0: 224x640 8 cars, 1 truck, 29.5ms\n",
      "Speed: 1.5ms preprocess, 29.5ms inference, 7.0ms postprocess per image at shape (1, 3, 224, 640)\n",
      "\n",
      "0: 224x640 8 cars, 3 trucks, 25.0ms\n",
      "Speed: 1.5ms preprocess, 25.0ms inference, 5.5ms postprocess per image at shape (1, 3, 224, 640)\n",
      "80 frames sono stati elaborati\n",
      "\n",
      "0: 224x640 8 cars, 3 trucks, 13.0ms\n",
      "Speed: 1.5ms preprocess, 13.0ms inference, 3.0ms postprocess per image at shape (1, 3, 224, 640)\n",
      "\n",
      "0: 224x640 7 cars, 4 trucks, 13.5ms\n",
      "Speed: 1.5ms preprocess, 13.5ms inference, 3.0ms postprocess per image at shape (1, 3, 224, 640)\n",
      "\n",
      "0: 224x640 7 cars, 3 trucks, 13.0ms\n",
      "Speed: 1.5ms preprocess, 13.0ms inference, 3.0ms postprocess per image at shape (1, 3, 224, 640)\n",
      "\n",
      "0: 224x640 8 cars, 3 trucks, 17.0ms\n",
      "Speed: 2.0ms preprocess, 17.0ms inference, 5.0ms postprocess per image at shape (1, 3, 224, 640)\n",
      "\n",
      "0: 224x640 9 cars, 3 trucks, 14.5ms\n",
      "Speed: 1.5ms preprocess, 14.5ms inference, 4.0ms postprocess per image at shape (1, 3, 224, 640)\n",
      "\n",
      "0: 224x640 9 cars, 3 trucks, 17.0ms\n",
      "Speed: 1.5ms preprocess, 17.0ms inference, 5.0ms postprocess per image at shape (1, 3, 224, 640)\n",
      "\n",
      "0: 224x640 8 cars, 3 trucks, 12.0ms\n",
      "Speed: 2.0ms preprocess, 12.0ms inference, 3.5ms postprocess per image at shape (1, 3, 224, 640)\n",
      "\n",
      "0: 224x640 8 cars, 3 trucks, 13.0ms\n",
      "Speed: 1.0ms preprocess, 13.0ms inference, 2.5ms postprocess per image at shape (1, 3, 224, 640)\n",
      "\n",
      "0: 224x640 8 cars, 3 trucks, 13.0ms\n",
      "Speed: 1.5ms preprocess, 13.0ms inference, 3.5ms postprocess per image at shape (1, 3, 224, 640)\n",
      "\n",
      "0: 224x640 9 cars, 3 trucks, 22.5ms\n",
      "Speed: 1.5ms preprocess, 22.5ms inference, 2.0ms postprocess per image at shape (1, 3, 224, 640)\n",
      "90 frames sono stati elaborati\n",
      "\n",
      "0: 224x640 9 cars, 3 trucks, 15.5ms\n",
      "Speed: 1.5ms preprocess, 15.5ms inference, 3.0ms postprocess per image at shape (1, 3, 224, 640)\n",
      "\n",
      "0: 224x640 10 cars, 3 trucks, 14.5ms\n",
      "Speed: 1.5ms preprocess, 14.5ms inference, 3.5ms postprocess per image at shape (1, 3, 224, 640)\n",
      "\n",
      "0: 224x640 8 cars, 2 trucks, 13.0ms\n",
      "Speed: 1.5ms preprocess, 13.0ms inference, 3.0ms postprocess per image at shape (1, 3, 224, 640)\n",
      "\n",
      "0: 224x640 7 cars, 3 trucks, 12.5ms\n",
      "Speed: 1.5ms preprocess, 12.5ms inference, 2.5ms postprocess per image at shape (1, 3, 224, 640)\n",
      "\n",
      "0: 224x640 7 cars, 2 trucks, 13.0ms\n",
      "Speed: 1.5ms preprocess, 13.0ms inference, 4.5ms postprocess per image at shape (1, 3, 224, 640)\n",
      "\n",
      "0: 224x640 9 cars, 2 trucks, 19.5ms\n",
      "Speed: 2.0ms preprocess, 19.5ms inference, 3.0ms postprocess per image at shape (1, 3, 224, 640)\n",
      "\n",
      "0: 224x640 7 cars, 2 trucks, 13.0ms\n",
      "Speed: 1.5ms preprocess, 13.0ms inference, 3.5ms postprocess per image at shape (1, 3, 224, 640)\n",
      "\n",
      "0: 224x640 9 cars, 2 trucks, 13.0ms\n",
      "Speed: 1.5ms preprocess, 13.0ms inference, 2.5ms postprocess per image at shape (1, 3, 224, 640)\n",
      "\n",
      "0: 224x640 8 cars, 2 trucks, 13.0ms\n",
      "Speed: 1.5ms preprocess, 13.0ms inference, 2.5ms postprocess per image at shape (1, 3, 224, 640)\n",
      "\n",
      "0: 224x640 7 cars, 1 truck, 13.5ms\n",
      "Speed: 1.5ms preprocess, 13.5ms inference, 4.5ms postprocess per image at shape (1, 3, 224, 640)\n",
      "100 frames sono stati elaborati\n",
      "\n",
      "0: 224x640 5 cars, 1 truck, 16.0ms\n",
      "Speed: 1.5ms preprocess, 16.0ms inference, 4.5ms postprocess per image at shape (1, 3, 224, 640)\n",
      "\n",
      "0: 224x640 6 cars, 1 truck, 28.0ms\n",
      "Speed: 1.5ms preprocess, 28.0ms inference, 5.0ms postprocess per image at shape (1, 3, 224, 640)\n",
      "\n",
      "0: 224x640 6 cars, 2 trucks, 14.0ms\n",
      "Speed: 1.5ms preprocess, 14.0ms inference, 3.0ms postprocess per image at shape (1, 3, 224, 640)\n",
      "\n",
      "0: 224x640 5 cars, 2 trucks, 17.5ms\n",
      "Speed: 1.5ms preprocess, 17.5ms inference, 4.0ms postprocess per image at shape (1, 3, 224, 640)\n",
      "\n",
      "0: 224x640 5 cars, 3 trucks, 14.0ms\n",
      "Speed: 1.5ms preprocess, 14.0ms inference, 3.0ms postprocess per image at shape (1, 3, 224, 640)\n",
      "\n",
      "0: 224x640 4 cars, 4 trucks, 13.0ms\n",
      "Speed: 1.5ms preprocess, 13.0ms inference, 3.5ms postprocess per image at shape (1, 3, 224, 640)\n",
      "\n",
      "0: 224x640 4 cars, 4 trucks, 13.0ms\n",
      "Speed: 1.5ms preprocess, 13.0ms inference, 3.0ms postprocess per image at shape (1, 3, 224, 640)\n",
      "\n",
      "0: 224x640 4 cars, 2 trucks, 12.0ms\n",
      "Speed: 1.5ms preprocess, 12.0ms inference, 3.0ms postprocess per image at shape (1, 3, 224, 640)\n",
      "\n",
      "0: 224x640 5 cars, 2 trucks, 13.5ms\n",
      "Speed: 1.5ms preprocess, 13.5ms inference, 2.5ms postprocess per image at shape (1, 3, 224, 640)\n",
      "\n",
      "0: 224x640 4 cars, 2 trucks, 13.5ms\n",
      "Speed: 2.0ms preprocess, 13.5ms inference, 4.0ms postprocess per image at shape (1, 3, 224, 640)\n",
      "110 frames sono stati elaborati\n",
      "\n",
      "0: 224x640 4 cars, 2 trucks, 12.0ms\n",
      "Speed: 2.0ms preprocess, 12.0ms inference, 3.5ms postprocess per image at shape (1, 3, 224, 640)\n",
      "\n",
      "0: 224x640 4 cars, 1 truck, 13.0ms\n",
      "Speed: 1.5ms preprocess, 13.0ms inference, 3.0ms postprocess per image at shape (1, 3, 224, 640)\n",
      "\n",
      "0: 224x640 5 cars, 1 truck, 13.5ms\n",
      "Speed: 1.5ms preprocess, 13.5ms inference, 4.0ms postprocess per image at shape (1, 3, 224, 640)\n",
      "\n",
      "0: 224x640 5 cars, 1 truck, 16.5ms\n",
      "Speed: 1.5ms preprocess, 16.5ms inference, 3.5ms postprocess per image at shape (1, 3, 224, 640)\n",
      "\n",
      "0: 224x640 4 cars, 1 truck, 13.0ms\n",
      "Speed: 1.5ms preprocess, 13.0ms inference, 2.0ms postprocess per image at shape (1, 3, 224, 640)\n",
      "\n",
      "0: 224x640 5 cars, 1 truck, 15.0ms\n",
      "Speed: 1.5ms preprocess, 15.0ms inference, 3.0ms postprocess per image at shape (1, 3, 224, 640)\n",
      "\n",
      "0: 224x640 5 cars, 1 truck, 12.5ms\n",
      "Speed: 1.5ms preprocess, 12.5ms inference, 4.0ms postprocess per image at shape (1, 3, 224, 640)\n",
      "\n",
      "0: 224x640 5 cars, 1 truck, 12.0ms\n",
      "Speed: 1.5ms preprocess, 12.0ms inference, 3.0ms postprocess per image at shape (1, 3, 224, 640)\n",
      "\n",
      "0: 224x640 5 cars, 2 trucks, 14.0ms\n",
      "Speed: 1.5ms preprocess, 14.0ms inference, 2.5ms postprocess per image at shape (1, 3, 224, 640)\n",
      "\n",
      "0: 224x640 5 cars, 2 trucks, 12.0ms\n",
      "Speed: 1.5ms preprocess, 12.0ms inference, 3.0ms postprocess per image at shape (1, 3, 224, 640)\n",
      "120 frames sono stati elaborati\n",
      "\n",
      "0: 224x640 4 cars, 1 truck, 13.0ms\n",
      "Speed: 1.5ms preprocess, 13.0ms inference, 2.0ms postprocess per image at shape (1, 3, 224, 640)\n",
      "\n",
      "0: 224x640 1 person, 4 cars, 1 truck, 12.0ms\n",
      "Speed: 1.5ms preprocess, 12.0ms inference, 2.5ms postprocess per image at shape (1, 3, 224, 640)\n",
      "\n",
      "0: 224x640 1 person, 4 cars, 1 truck, 13.0ms\n",
      "Speed: 2.0ms preprocess, 13.0ms inference, 3.0ms postprocess per image at shape (1, 3, 224, 640)\n",
      "\n",
      "0: 224x640 1 person, 4 cars, 2 trucks, 12.0ms\n",
      "Speed: 1.5ms preprocess, 12.0ms inference, 4.0ms postprocess per image at shape (1, 3, 224, 640)\n",
      "\n",
      "0: 224x640 1 person, 4 cars, 1 truck, 12.5ms\n",
      "Speed: 1.5ms preprocess, 12.5ms inference, 3.5ms postprocess per image at shape (1, 3, 224, 640)\n",
      "\n",
      "0: 224x640 1 person, 4 cars, 13.0ms\n",
      "Speed: 2.0ms preprocess, 13.0ms inference, 3.0ms postprocess per image at shape (1, 3, 224, 640)\n",
      "\n",
      "0: 224x640 1 person, 4 cars, 16.0ms\n",
      "Speed: 1.5ms preprocess, 16.0ms inference, 3.0ms postprocess per image at shape (1, 3, 224, 640)\n",
      "\n",
      "0: 224x640 5 cars, 13.0ms\n",
      "Speed: 1.5ms preprocess, 13.0ms inference, 3.0ms postprocess per image at shape (1, 3, 224, 640)\n",
      "\n",
      "0: 224x640 5 cars, 13.0ms\n",
      "Speed: 1.5ms preprocess, 13.0ms inference, 3.0ms postprocess per image at shape (1, 3, 224, 640)\n",
      "Tutti i frames sono stati elaborati\n",
      "renders\\output2.ply\n"
     ]
    }
   ],
   "source": [
    "path = \"videoreferences/2011_09_26_drive_0035_sync\"\n",
    "\n",
    "data_handler = DataLoader(path)\n",
    "\n",
    "trajectory = odometry(data_handler)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
