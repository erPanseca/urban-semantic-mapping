{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "import cv2\n",
    "from ultralytics import YOLO\n",
    "import torch\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import torchvision as tv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "yolo_model = YOLO(\"yolov8n.pt\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_midas_model():\n",
    "    model_type = \"DPT_Large\"\n",
    "    model = torch.hub.load(\"intel-isl/MiDaS\", model_type)\n",
    "    model.eval()\n",
    "    return model\n",
    "\n",
    "def get_depth_map(model, image):\n",
    "    transform = tv.Compose([\n",
    "        tv.Resize(384, 384),\n",
    "        tv.ToTensor(),\n",
    "        tv.Normalize(mean = [0.5], std = [0.5]),\n",
    "    ]) \n",
    "    input_tensor = transform(image).unsqueeze(0)\n",
    "    with torch.no_grad():\n",
    "        depth_map = model(input_tensor)\n",
    "    depth_map = depth_map.squeeze().numpy()\n",
    "    return depth_map"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'load_midas_model' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m/tmp/ipykernel_27556/88519541.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mmidas_model\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mload_midas_model\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m: name 'load_midas_model' is not defined"
     ]
    }
   ],
   "source": [
    "midas_model = load_midas_model()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def detect_objects(frame):\n",
    "    detection_info = []\n",
    "    #carica l'immagine\n",
    "    img = cv2.imread(frame)\n",
    "\n",
    "    #esegui il rilevamento con yolov8\n",
    "    yolo_results = yolo_model(img)\n",
    "\n",
    "    #ottieni la mappa di profondita' con MiDaS\n",
    "    depth_map = get_depth_map(midas_model, frame)\n",
    "\n",
    "    #ottieni i risultati del rilevamento (results[0] indica le informazioni ottenute dall'unica immagine in input, se ci fossero piu di una immagine allora sarebbe results[i] con i = 1...n)\n",
    "    detection = yolo_results[0].boxes.xyxy #coordinate della bounding box\n",
    "    confidences = yolo_results[0].boxes.conf #confidenza della rilevazione\n",
    "    class_ids = yolo_results[0].boxes.cls #ID delle classi di appartenenza dell'oggetto\n",
    "\n",
    "    #visualizza i risultati\n",
    "    for i, (box, conf, cls_id) in enumerate(zip(detection, confidences, class_ids)):\n",
    "        x1, y1, x2, y2 = map(int, box) #estrai le coordinate della bounding box\n",
    "        label = f\"{yolo_model.names[int(cls_id)]} ({conf:.2f})\" #viene ncreata un'etichetta contenente il nome della classe (poiche class_ids e' un intero che grazie a model.names[cls_id] viene convertito nella corrispondente classe) e la confidenza\n",
    "\n",
    "        #calcola la profondita' media nella bounding box\n",
    "        depth_in_box = depth_map[y1:y2, x1:x2]\n",
    "        average_depth = np.mean(depth_in_box) #media della profondita'\n",
    "\n",
    "        detection_info.append({\n",
    "            'frame': capture.get(cv2.CAP_PROP_POS_FRAMES),\n",
    "            'label': label,\n",
    "            'confidence': conf,\n",
    "            'x1': x1,\n",
    "            'x2': x2,\n",
    "            'y1': y1,\n",
    "            'y2': y2,\n",
    "            'average_depth': average_depth\n",
    "        })\n",
    "\n",
    "        #cv2.rectangle(img, (x1, y1), (x2, y2), (0, 255, 0), 2)\n",
    "        #cv2.putText(img, label, (x1, y1 - 10), cv2.FONT_HERSHEY_COMPLEX, 0.9, (255, 0, 0), 2)\n",
    "    return detection_info\n",
    "\n",
    "    #mostra l'immagine con i bounding box\n",
    "    #cv2.imshow(\"Detections\", img)\n",
    "    #cv2.waitKey(0)\n",
    "    #cv2.destroyAllWindows()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "video_path = \"\"\n",
    "capture = cv2.VideoCapture(video_path)\n",
    "detection_info = []\n",
    "while capture.isOpened():\n",
    "    ret, frame = capture.read()\n",
    "    if not ret:\n",
    "        break #esci dal ciclo se non ci sono piu frame\n",
    "    detection_info.append(detect_objects(frame))\n",
    "capture.release()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "0: 448x640 17 cars, 2 trucks, 292.5ms\n",
      "Speed: 3.0ms preprocess, 292.5ms inference, 1.8ms postprocess per image at shape (1, 3, 448, 640)\n"
     ]
    }
   ],
   "source": [
    "detect_objects(\"./references/autostrada.jpg\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
